{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "from random import *\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test  = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(\n",
    "    [\n",
    "        pd.get_dummies(train['letter']).values.reshape(-1, 1, 26),\n",
    "        (train[[str(i) for i in range(784)]] / 255.).values.reshape(-1, 1, 784)\n",
    "    ],\n",
    "    axis=2\n",
    ")\n",
    "y_train = train['digit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor로 형변환\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_valid = torch.Tensor(x_valid)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_valid = torch.LongTensor(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(\n",
    "    x_train[:, :, :26], # Letter\n",
    "    x_train[:, :, 26:].reshape(-1, 1, 28, 28), # Image\n",
    "    y_train # Digit\n",
    ")\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=16)\n",
    "\n",
    "valid_data = TensorDataset(\n",
    "    x_valid[:, :, :26],\n",
    "    x_valid[:, :, 26:].reshape(-1, 1, 28, 28),\n",
    "    y_valid\n",
    ")\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Letter의 Convolution Block\n",
    "        self.layer1_1 = self.conv_module_1d(1, 16)\n",
    "        self.layer1_2 = self.conv_module_1d(16, 32)\n",
    "        self.layer1_3 = self.conv_module_1d(32, 64)\n",
    "        self.layer1_4 = self.conv_module_1d(64, 128)\n",
    " \n",
    "        # Image의 Convolution Block\n",
    "        self.layer2_1 = self.conv_module_2d(1, 16)\n",
    "        self.layer2_2 = self.conv_module_2d(16, 32)\n",
    "        self.layer2_3 = self.conv_module_2d(32, 64)\n",
    "        self.layer2_4 = self.conv_module_2d(64, 128)\n",
    "        self.layer2_5 = self.conv_module_2d(128, 256)\n",
    "        self.layey2_6 = self.conv_module_2d(256, 512)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(204032, 1024).cuda(), nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 128).cuda(), nn.LeakyReLU(),\n",
    "            nn.Linear(128, 32).cuda(), nn.LeakyReLU(),\n",
    "            nn.Linear(32, 10).cuda()\n",
    "        )\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x1, x2, label=False):\n",
    "        out = self._inference(x1, x2)\n",
    "        if label is not False:\n",
    "            loss = self.loss(out, label)\n",
    "            return (out, loss)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _inference(self, x1, x2):\n",
    "        bsz1 = x1.size(0)\n",
    "        bsz2 = x2.size(0)\n",
    "        \n",
    "#         print(bsz1, bsz2)\n",
    "\n",
    "        x1 = self.layer1_1(x1)\n",
    "        x1 = self.layer1_2(x1)\n",
    "        x1 = self.layer1_3(x1)\n",
    "        x1 = self.layer1_4(x1)\n",
    "        \n",
    "        x2 = self.layer2_1(x2)\n",
    "        x2 = self.layer2_2(x2)\n",
    "        x2 = self.layer2_3(x2)\n",
    "        x2 = self.layer2_4(x2)\n",
    "        x2 = self.layer2_5(x2)\n",
    "        \n",
    "#         print(x1.shape)\n",
    "#         print(x2.shape)\n",
    "        \n",
    "        x1 = x1.view(bsz1, -1)\n",
    "        x2 = x2.view(bsz2, -1)\n",
    "        \n",
    "#         print(x1.shape)\n",
    "#         print(x2.shape)\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "#         print(x.shape)\n",
    "        out = torch.nn.functional.softmax(self.out(x), dim=1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def conv_module_1d(self, in_num, out_num):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_num, out_num, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(out_num),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(1)\n",
    "        )\n",
    "    \n",
    "    def conv_module_2d(self, in_num, out_num):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_num, out_num, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_num),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d((1, 1))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customCNN(\n",
       "  (layer1_1): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer1_2): Sequential(\n",
       "    (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer1_3): Sequential(\n",
       "    (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer1_4): Sequential(\n",
       "    (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2_1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2_2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2_3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2_4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2_5): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layey2_6): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=204032, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = customCNN()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_letter = x_train[:32, :, :26].cuda()\n",
    "test_image = x_train[:32, :, 26:].reshape(-1, 1, 28, 28).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 26])\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(test_letter.shape)\n",
    "print(test_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1118, 0.1026, 0.0900, 0.0892, 0.1000, 0.0957, 0.0983, 0.0957, 0.0974,\n",
       "         0.1193],\n",
       "        [0.1136, 0.1092, 0.0880, 0.0872, 0.1012, 0.0941, 0.0948, 0.0957, 0.0991,\n",
       "         0.1171],\n",
       "        [0.1156, 0.1077, 0.0907, 0.0860, 0.0986, 0.0929, 0.0948, 0.0943, 0.0974,\n",
       "         0.1220],\n",
       "        [0.1142, 0.1053, 0.0922, 0.0871, 0.0964, 0.0950, 0.0971, 0.0970, 0.1001,\n",
       "         0.1157],\n",
       "        [0.1122, 0.1087, 0.0907, 0.0876, 0.0969, 0.0934, 0.1023, 0.0945, 0.1009,\n",
       "         0.1129],\n",
       "        [0.1156, 0.1069, 0.0892, 0.0855, 0.0978, 0.0958, 0.0991, 0.0950, 0.0985,\n",
       "         0.1166],\n",
       "        [0.1127, 0.1101, 0.0911, 0.0869, 0.0990, 0.0950, 0.0972, 0.0940, 0.1003,\n",
       "         0.1135],\n",
       "        [0.1157, 0.1082, 0.0909, 0.0875, 0.0948, 0.0938, 0.0971, 0.0955, 0.1010,\n",
       "         0.1155],\n",
       "        [0.1161, 0.1085, 0.0911, 0.0865, 0.0960, 0.0938, 0.1006, 0.0950, 0.0999,\n",
       "         0.1126],\n",
       "        [0.1161, 0.1082, 0.0895, 0.0885, 0.0967, 0.0953, 0.0973, 0.0954, 0.0988,\n",
       "         0.1143],\n",
       "        [0.1164, 0.1083, 0.0917, 0.0863, 0.0962, 0.0944, 0.0996, 0.0954, 0.0984,\n",
       "         0.1133],\n",
       "        [0.1149, 0.1074, 0.0896, 0.0878, 0.0961, 0.0935, 0.0996, 0.0969, 0.0997,\n",
       "         0.1145],\n",
       "        [0.1149, 0.1077, 0.0902, 0.0880, 0.0951, 0.0951, 0.1000, 0.0952, 0.1001,\n",
       "         0.1138],\n",
       "        [0.1142, 0.1071, 0.0933, 0.0884, 0.0979, 0.0951, 0.0961, 0.0926, 0.1014,\n",
       "         0.1140],\n",
       "        [0.1150, 0.1055, 0.0906, 0.0863, 0.0976, 0.0977, 0.0955, 0.0954, 0.0983,\n",
       "         0.1180],\n",
       "        [0.1125, 0.1086, 0.0887, 0.0871, 0.0968, 0.0950, 0.1001, 0.0948, 0.1014,\n",
       "         0.1149],\n",
       "        [0.1140, 0.1082, 0.0894, 0.0875, 0.0973, 0.0928, 0.1003, 0.0956, 0.1006,\n",
       "         0.1143],\n",
       "        [0.1159, 0.1082, 0.0912, 0.0858, 0.0952, 0.0949, 0.0998, 0.0940, 0.1013,\n",
       "         0.1138],\n",
       "        [0.1134, 0.1085, 0.0900, 0.0852, 0.0973, 0.0952, 0.0996, 0.0961, 0.0990,\n",
       "         0.1157],\n",
       "        [0.1140, 0.1115, 0.0867, 0.0870, 0.0966, 0.0921, 0.1017, 0.0959, 0.1014,\n",
       "         0.1131],\n",
       "        [0.1159, 0.1106, 0.0908, 0.0881, 0.0948, 0.0945, 0.0953, 0.0932, 0.1022,\n",
       "         0.1146],\n",
       "        [0.1154, 0.1077, 0.0882, 0.0870, 0.0979, 0.0951, 0.0978, 0.0949, 0.1013,\n",
       "         0.1146],\n",
       "        [0.1139, 0.1102, 0.0898, 0.0880, 0.0969, 0.0928, 0.0989, 0.0950, 0.1005,\n",
       "         0.1139],\n",
       "        [0.1140, 0.1067, 0.0920, 0.0885, 0.0977, 0.0938, 0.0963, 0.0951, 0.0995,\n",
       "         0.1164],\n",
       "        [0.1107, 0.1069, 0.0909, 0.0861, 0.0993, 0.0979, 0.0979, 0.0957, 0.0972,\n",
       "         0.1173],\n",
       "        [0.1140, 0.1083, 0.0891, 0.0868, 0.0963, 0.0977, 0.0964, 0.0962, 0.0990,\n",
       "         0.1161],\n",
       "        [0.1189, 0.1074, 0.0911, 0.0853, 0.0961, 0.0949, 0.0975, 0.0940, 0.1012,\n",
       "         0.1137],\n",
       "        [0.1162, 0.1101, 0.0889, 0.0884, 0.0936, 0.0955, 0.0969, 0.0945, 0.1045,\n",
       "         0.1113],\n",
       "        [0.1120, 0.1099, 0.0898, 0.0886, 0.0969, 0.0935, 0.0983, 0.0964, 0.1017,\n",
       "         0.1128],\n",
       "        [0.1154, 0.1051, 0.0904, 0.0856, 0.0976, 0.0949, 0.0999, 0.0937, 0.1032,\n",
       "         0.1141],\n",
       "        [0.1157, 0.1075, 0.0886, 0.0876, 0.0971, 0.0963, 0.0981, 0.0927, 0.1030,\n",
       "         0.1135],\n",
       "        [0.1141, 0.1088, 0.0894, 0.0868, 0.0950, 0.0959, 0.0987, 0.0954, 0.1015,\n",
       "         0.1145]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_letter, test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta(model.parameters(), lr=0.1)\n",
    "epochs = 200\n",
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 798.00 MiB (GPU 0; 10.76 GiB total capacity; 8.48 GiB already allocated; 408.50 MiB free; 8.53 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-69fd21ff8fc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# gradient를 통해 weight update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# gradient 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adadelta.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_delta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 798.00 MiB (GPU 0; 10.76 GiB total capacity; 8.48 GiB already allocated; 408.50 MiB free; 8.53 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# gradient 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "history = defaultdict(list)\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    # train 모드로 변경\n",
    "    model.train()\n",
    "    \n",
    "    # dataloader에서 batch size만큼 반복해서 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # batch를 GPU에 적용\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # batch에서 데이터 추출\n",
    "        letter, image, label = batch\n",
    "        \n",
    "        # Forward Propagation 수행\n",
    "        outputs = model(letter, image, label)\n",
    "        \n",
    "        loss = outputs[1]\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward Propagation 수행\n",
    "        loss.backward()\n",
    "        history[\"train_loss\"].append(loss.item())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "        label = label.to(\"cpu\").numpy()\n",
    "        tmp_train_accuracy = flat_accuracy(logits, label)\n",
    "        history[\"train_acc\"].append(tmp_train_accuracy)\n",
    "        \n",
    "        # Gradient Cleeping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # gradient를 통해 weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gradient 초기화\n",
    "        model.zero_grad()\n",
    "        \n",
    "    # average loss\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # eval 모드로 변경\n",
    "    model.eval()\n",
    "    \n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy, nb_eval_steps, nb_eval_examples = 0, 0, 0, 0\n",
    "    \n",
    "    # dataloader에서 batch만큼 반복해서 가져옴\n",
    "    for batch in valid_dataloader:\n",
    "        \n",
    "        # batch를 GPU에 적용\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # batch에서 데이터 추출\n",
    "        letter, image, label = batch\n",
    "        \n",
    "        # gradient 계산 안함\n",
    "        with torch.no_grad():\n",
    "            # Forward Propagation 수행\n",
    "            outputs = model(letter, image, label)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        history[\"eval_loss\"].append(outputs[1].item())\n",
    "        \n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label = label.to(\"cpu\").numpy()\n",
    "        \n",
    "        # 출력 logit과 label을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label)\n",
    "        history[\"eval_acc\"].append(tmp_eval_accuracy)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    s = f\"\\r[Epoch {epoch_i+1}/{epochs}]\"\n",
    "    s += f\" Avg Training Loss: {avg_train_loss: .2f}\"\n",
    "    s += \" Valid Acc: {0:.2f}\".format(eval_accuracy / nb_eval_steps)\n",
    "    print(s, end=\"\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model/emnist_model4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = customCNN()\n",
    "model.load_state_dict(torch.load(\"./model/emnist_model4.pt\"))\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.concatenate(\n",
    "    [\n",
    "        pd.get_dummies(test[\"letter\"]).values.reshape(-1, 1, 26),\n",
    "        (test[[str(i) for i in range(784)]] / 255.).values.reshape(-1, 1, 784)\n",
    "    ],\n",
    "    axis=2\n",
    ")\n",
    "x_test = torch.Tensor(x_test)\n",
    "\n",
    "x1 = x_test[:, :, :26].cuda()\n",
    "x2 = x_test[:, :, 26:].reshape(-1, 1, 28, 28).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(x1, x2)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for batch in test_dataloader:\n",
    "    input1, input2 = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input1, input2)\n",
    "    y_pred.append(torch.argmax(outputs, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"digit\"] = torch.cat(y_pred).detach().cpu().numpy()\n",
    "submission.to_csv(\"./result/submission4.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
