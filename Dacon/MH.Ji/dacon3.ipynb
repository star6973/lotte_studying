{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "from random import *\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test  = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "      <th>letter</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 787 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  digit letter  0  1  2  3  4  5  6  ...  774  775  776  777  778  779  \\\n",
       "0   1      5      L  1  1  1  4  3  0  0  ...    2    1    0    1    2    4   \n",
       "1   2      0      B  0  4  0  0  4  1  1  ...    0    3    0    1    4    1   \n",
       "2   3      4      L  1  1  2  2  1  1  1  ...    3    3    3    0    2    0   \n",
       "3   4      9      D  1  2  0  2  0  4  0  ...    3    3    2    0    1    4   \n",
       "4   5      6      A  3  0  2  4  0  3  0  ...    4    4    3    2    1    3   \n",
       "\n",
       "   780  781  782  783  \n",
       "0    4    4    3    4  \n",
       "1    4    2    1    2  \n",
       "2    3    0    2    2  \n",
       "3    0    0    1    1  \n",
       "4    4    3    1    2  \n",
       "\n",
       "[5 rows x 787 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(\n",
    "    [\n",
    "        pd.get_dummies(train['letter']).values.reshape(-1, 1, 26),\n",
    "        (train[[str(i) for i in range(784)]] / 255.).values.reshape(-1, 1, 784)\n",
    "    ],\n",
    "    axis=2\n",
    ")\n",
    "y_train = train['digit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01568627, 0.01176471, 0.00784314, 0.        ,\n",
       "        0.01568627, 0.        , 0.00784314, 0.00784314, 0.01176471,\n",
       "        0.01568627, 0.        , 0.        , 0.00392157, 0.01176471,\n",
       "        0.00392157, 0.00784314, 0.01568627, 0.00784314, 0.01568627,\n",
       "        0.00392157, 0.00392157, 0.00392157, 0.01568627, 0.00392157,\n",
       "        0.        , 0.00784314, 0.00392157, 0.01568627, 0.00784314,\n",
       "        0.01176471, 0.00784314, 0.        , 0.        , 0.01568627,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.01568627,\n",
       "        0.00784314, 0.01568627, 0.        , 0.        , 0.00392157,\n",
       "        0.00392157, 0.00392157, 0.01176471, 0.01176471, 0.00392157,\n",
       "        0.00784314, 0.01176471, 0.        , 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.01176471, 0.00392157, 0.00392157, 0.00392157,\n",
       "        0.01176471, 0.01176471, 0.00392157, 0.00784314, 0.01568627,\n",
       "        0.01568627, 0.00392157, 0.00784314, 0.01176471, 0.        ,\n",
       "        0.        , 0.01568627, 0.00784314, 0.01568627, 0.25490196,\n",
       "        0.18823529, 0.1372549 , 0.1372549 , 0.05490196, 0.00392157,\n",
       "        0.00784314, 0.00784314, 0.01568627, 0.00392157, 0.00392157,\n",
       "        0.00392157, 0.01176471, 0.        , 0.01568627, 0.00392157,\n",
       "        0.01176471, 0.00392157, 0.01176471, 0.00392157, 0.        ,\n",
       "        0.00392157, 0.01176471, 0.01176471, 0.01176471, 0.        ,\n",
       "        0.00784314, 0.50980392, 0.54117647, 0.54901961, 0.65098039,\n",
       "        0.63921569, 0.28627451, 0.01176471, 0.01176471, 0.        ,\n",
       "        0.00784314, 0.01568627, 0.        , 0.00784314, 0.        ,\n",
       "        0.01568627, 0.00784314, 0.00392157, 0.01568627, 0.00784314,\n",
       "        0.01176471, 0.        , 0.01176471, 0.01176471, 0.01176471,\n",
       "        0.        , 0.01568627, 0.52156863, 0.55294118, 0.65098039,\n",
       "        0.83921569, 0.91372549, 0.98039216, 0.91372549, 0.53333333,\n",
       "        0.00784314, 0.01568627, 0.00392157, 0.01176471, 0.00392157,\n",
       "        0.01176471, 0.        , 0.00392157, 0.        , 0.00784314,\n",
       "        0.01176471, 0.00392157, 0.        , 0.00784314, 0.01568627,\n",
       "        0.01568627, 0.00784314, 0.01176471, 0.41568627, 0.47843137,\n",
       "        0.61568627, 0.6627451 , 0.72156863, 0.74509804, 0.82745098,\n",
       "        0.85882353, 0.86666667, 0.56470588, 0.01176471, 0.00784314,\n",
       "        0.01568627, 0.        , 0.00784314, 0.01176471, 0.01568627,\n",
       "        0.01568627, 0.        , 0.        , 0.00392157, 0.00784314,\n",
       "        0.00392157, 0.00392157, 0.        , 0.        , 0.00784314,\n",
       "        0.03137255, 0.12941176, 0.22745098, 0.42745098, 0.47058824,\n",
       "        0.50196078, 0.50980392, 0.54117647, 0.59607843, 0.64313725,\n",
       "        0.55294118, 0.50980392, 0.01176471, 0.01176471, 0.01568627,\n",
       "        0.00784314, 0.01176471, 0.00392157, 0.01176471, 0.01568627,\n",
       "        0.00392157, 0.00784314, 0.01176471, 0.00784314, 0.01568627,\n",
       "        0.00392157, 0.00392157, 0.01568627, 0.14117647, 0.35294118,\n",
       "        0.43137255, 0.49019608, 0.50980392, 0.51372549, 0.49803922,\n",
       "        0.56470588, 0.80784314, 0.91372549, 0.83529412, 0.61960784,\n",
       "        0.35686275, 0.        , 0.01568627, 0.        , 0.00392157,\n",
       "        0.01568627, 0.00784314, 0.01176471, 0.01176471, 0.01176471,\n",
       "        0.01176471, 0.00784314, 0.01568627, 0.00392157, 0.01176471,\n",
       "        0.03137255, 0.25490196, 0.45882353, 0.48235294, 0.50588235,\n",
       "        0.50196078, 0.49803922, 0.52941176, 0.75294118, 0.98431373,\n",
       "        0.99215686, 0.94509804, 0.62352941, 0.16470588, 0.        ,\n",
       "        0.        , 0.01176471, 0.00392157, 0.00392157, 0.00392157,\n",
       "        0.01176471, 0.01568627, 0.01176471, 0.        , 0.00392157,\n",
       "        0.        , 0.01960784, 0.12941176, 0.24313725, 0.44313725,\n",
       "        0.50588235, 0.50196078, 0.4745098 , 0.50588235, 0.5254902 ,\n",
       "        0.68235294, 0.90196078, 0.99607843, 0.98823529, 0.93333333,\n",
       "        0.51764706, 0.18039216, 0.01568627, 0.00784314, 0.00392157,\n",
       "        0.01176471, 0.        , 0.01176471, 0.01176471, 0.00392157,\n",
       "        0.00784314, 0.01176471, 0.00784314, 0.01568627, 0.05490196,\n",
       "        0.3372549 , 0.46666667, 0.50588235, 0.4745098 , 0.41960784,\n",
       "        0.38823529, 0.58431373, 0.5372549 , 0.59607843, 0.6745098 ,\n",
       "        0.74117647, 0.64313725, 0.63921569, 0.43921569, 0.25882353,\n",
       "        0.02352941, 0.00392157, 0.00392157, 0.01176471, 0.00784314,\n",
       "        0.01176471, 0.01568627, 0.00392157, 0.01176471, 0.00392157,\n",
       "        0.        , 0.05098039, 0.11372549, 0.41960784, 0.58039216,\n",
       "        0.70588235, 0.6627451 , 0.64313725, 0.58039216, 0.67058824,\n",
       "        0.56862745, 0.5254902 , 0.54509804, 0.52941176, 0.28627451,\n",
       "        0.47843137, 0.44705882, 0.29019608, 0.01960784, 0.01568627,\n",
       "        0.00392157, 0.00392157, 0.        , 0.00392157, 0.00392157,\n",
       "        0.01568627, 0.01176471, 0.00784314, 0.02745098, 0.24313725,\n",
       "        0.4       , 0.66666667, 0.88627451, 0.80392157, 0.58431373,\n",
       "        0.54117647, 0.50980392, 0.50980392, 0.00784314, 0.01176471,\n",
       "        0.01176471, 0.47058824, 0.27843137, 0.46666667, 0.47843137,\n",
       "        0.38823529, 0.0627451 , 0.00392157, 0.00784314, 0.01568627,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.00392157, 0.16078431, 0.44705882, 0.54117647, 0.80392157,\n",
       "        0.83137255, 0.58039216, 0.51372549, 0.01176471, 0.00784314,\n",
       "        0.        , 0.01568627, 0.00784314, 0.00784314, 0.50980392,\n",
       "        0.5372549 , 0.63921569, 0.43921569, 0.2745098 , 0.01960784,\n",
       "        0.01176471, 0.00392157, 0.00784314, 0.00784314, 0.00784314,\n",
       "        0.00784314, 0.01176471, 0.01568627, 0.01960784, 0.25882353,\n",
       "        0.47843137, 0.51764706, 0.6       , 0.67843137, 0.50980392,\n",
       "        0.01176471, 0.01568627, 0.00392157, 0.00784314, 0.00392157,\n",
       "        0.00392157, 0.01568627, 0.4745098 , 0.60784314, 0.78039216,\n",
       "        0.47058824, 0.24705882, 0.01960784, 0.        , 0.01176471,\n",
       "        0.00392157, 0.01176471, 0.00392157, 0.00392157, 0.01176471,\n",
       "        0.02352941, 0.09803922, 0.41568627, 0.50980392, 0.49411765,\n",
       "        0.3254902 , 0.30196078, 0.00392157, 0.00392157, 0.01568627,\n",
       "        0.01176471, 0.        , 0.00784314, 0.01176471, 0.10980392,\n",
       "        0.56470588, 0.81176471, 0.89411765, 0.40784314, 0.16862745,\n",
       "        0.01568627, 0.01568627, 0.01176471, 0.01568627, 0.        ,\n",
       "        0.        , 0.01568627, 0.00784314, 0.07058824, 0.22352941,\n",
       "        0.49019608, 0.4745098 , 0.41960784, 0.14901961, 0.05882353,\n",
       "        0.00392157, 0.        , 0.00392157, 0.00392157, 0.01568627,\n",
       "        0.01176471, 0.11372549, 0.42352941, 0.90196078, 0.97254902,\n",
       "        0.78039216, 0.19215686, 0.04313725, 0.01568627, 0.00392157,\n",
       "        0.00392157, 0.01176471, 0.        , 0.00392157, 0.00392157,\n",
       "        0.01568627, 0.10980392, 0.27843137, 0.49803922, 0.38039216,\n",
       "        0.25882353, 0.0627451 , 0.01176471, 0.01176471, 0.01568627,\n",
       "        0.00392157, 0.00392157, 0.        , 0.19215686, 0.50196078,\n",
       "        0.69411765, 0.96470588, 0.9372549 , 0.56078431, 0.0745098 ,\n",
       "        0.02352941, 0.01176471, 0.00784314, 0.01568627, 0.00392157,\n",
       "        0.00392157, 0.        , 0.00784314, 0.02352941, 0.22352941,\n",
       "        0.4       , 0.49803922, 0.26666667, 0.09411765, 0.00784314,\n",
       "        0.01176471, 0.        , 0.00392157, 0.01176471, 0.08235294,\n",
       "        0.34117647, 0.56862745, 0.8745098 , 0.95686275, 0.94117647,\n",
       "        0.63137255, 0.19215686, 0.01960784, 0.01176471, 0.01176471,\n",
       "        0.00392157, 0.        , 0.01568627, 0.01176471, 0.00392157,\n",
       "        0.00392157, 0.03137255, 0.3254902 , 0.45882353, 0.49803922,\n",
       "        0.28627451, 0.10980392, 0.02352941, 0.01568627, 0.03921569,\n",
       "        0.10588235, 0.17647059, 0.50196078, 0.7372549 , 0.91372549,\n",
       "        0.98431373, 0.94509804, 0.57254902, 0.22745098, 0.01960784,\n",
       "        0.00784314, 0.01568627, 0.00392157, 0.01176471, 0.        ,\n",
       "        0.01176471, 0.01568627, 0.01568627, 0.01176471, 0.03921569,\n",
       "        0.31764706, 0.45098039, 0.49411765, 0.34901961, 0.20392157,\n",
       "        0.09803922, 0.17647059, 0.32941176, 0.56862745, 0.69019608,\n",
       "        0.82745098, 0.91764706, 0.97254902, 0.94901961, 0.74117647,\n",
       "        0.22352941, 0.08235294, 0.00392157, 0.00784314, 0.00392157,\n",
       "        0.01568627, 0.00784314, 0.01568627, 0.00392157, 0.00784314,\n",
       "        0.01176471, 0.01568627, 0.04313725, 0.23137255, 0.36078431,\n",
       "        0.51764706, 0.55686275, 0.56862745, 0.67058824, 0.83529412,\n",
       "        0.92156863, 0.98823529, 1.        , 0.99215686, 0.98823529,\n",
       "        0.89019608, 0.50588235, 0.23137255, 0.02352941, 0.00392157,\n",
       "        0.        , 0.00784314, 0.00392157, 0.01568627, 0.01176471,\n",
       "        0.00784314, 0.00392157, 0.00392157, 0.00784314, 0.01568627,\n",
       "        0.00784314, 0.49411765, 0.53333333, 0.76862745, 0.91372549,\n",
       "        0.94117647, 0.98039216, 0.99215686, 0.99607843, 1.        ,\n",
       "        1.        , 0.98431373, 0.81176471, 0.49411765, 0.20392157,\n",
       "        0.07843137, 0.01176471, 0.        , 0.01176471, 0.01176471,\n",
       "        0.01568627, 0.00784314, 0.00784314, 0.01568627, 0.01568627,\n",
       "        0.00392157, 0.01568627, 0.        , 0.        , 0.49411765,\n",
       "        0.55686275, 0.81176471, 0.96470588, 0.98039216, 1.        ,\n",
       "        1.        , 0.97647059, 0.92156863, 0.85490196, 0.65098039,\n",
       "        0.38431373, 0.09803922, 0.02352941, 0.00392157, 0.00392157,\n",
       "        0.01568627, 0.01568627, 0.01568627, 0.00784314, 0.01568627,\n",
       "        0.01176471, 0.00392157, 0.00392157, 0.01176471, 0.        ,\n",
       "        0.00784314, 0.00392157, 0.01176471, 0.48235294, 0.54117647,\n",
       "        0.65098039, 0.71764706, 0.71764706, 0.65882353, 0.49803922,\n",
       "        0.32941176, 0.22352941, 0.08627451, 0.04705882, 0.00392157,\n",
       "        0.        , 0.        , 0.00392157, 0.01176471, 0.01568627,\n",
       "        0.        , 0.01568627, 0.        , 0.00784314, 0.01176471,\n",
       "        0.01568627, 0.01176471, 0.00392157, 0.01176471, 0.        ,\n",
       "        0.01568627, 0.01176471, 0.25098039, 0.30196078, 0.31764706,\n",
       "        0.23137255, 0.1372549 , 0.10196078, 0.07058824, 0.05098039,\n",
       "        0.02352941, 0.01960784, 0.01176471, 0.00392157, 0.01176471,\n",
       "        0.01176471, 0.00784314, 0.00392157, 0.01568627, 0.        ,\n",
       "        0.01568627, 0.        , 0.00784314, 0.        , 0.01176471,\n",
       "        0.01568627, 0.01176471, 0.00784314, 0.00784314, 0.        ,\n",
       "        0.01568627, 0.01176471, 0.01568627, 0.01568627, 0.        ,\n",
       "        0.        , 0.00784314, 0.01176471, 0.        , 0.01568627,\n",
       "        0.01568627, 0.01568627, 0.00392157, 0.        , 0.00784314,\n",
       "        0.01176471, 0.01568627, 0.00784314, 0.00784314, 0.01568627,\n",
       "        0.00784314, 0.01568627, 0.        , 0.01568627, 0.00784314,\n",
       "        0.        , 0.01176471, 0.00784314, 0.00392157, 0.00784314,\n",
       "        0.00784314, 0.01176471, 0.01568627, 0.01568627, 0.00784314,\n",
       "        0.01176471, 0.        , 0.00784314, 0.        , 0.01176471,\n",
       "        0.01176471, 0.01176471, 0.01568627, 0.01176471, 0.00784314,\n",
       "        0.00784314, 0.01568627, 0.        , 0.01176471, 0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1638, 1, 810)\n",
      "(410, 1, 810)\n",
      "(1638,)\n",
      "(410,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor로 형변환\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_valid = torch.Tensor(x_valid)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_valid = torch.LongTensor(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1638, 1, 810])\n",
      "torch.Size([410, 1, 810])\n",
      "torch.Size([1638])\n",
      "torch.Size([410])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(\n",
    "    x_train[:, :, :26], # Letter\n",
    "    x_train[:, :, 26:].reshape(-1, 1, 28, 28), # Image\n",
    "    y_train # Digit\n",
    ")\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "valid_data = TensorDataset(\n",
    "    x_valid[:, :, :26],\n",
    "    x_valid[:, :, 26:].reshape(-1, 1, 28, 28),\n",
    "    y_valid\n",
    ")\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter, image, digit in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 26])\n",
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(letter.shape)\n",
    "print(image.shape)\n",
    "print(digit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Letter의 Convolution Block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 3, padding=1), nn.ReLU(), # 16@26\n",
    "            nn.Conv1d(16, 64, 4, padding=1), nn.ReLU(), # 64@25\n",
    "            nn.Conv1d(64, 128, 5, padding=2), nn.ReLU(), # 128@25\n",
    "            nn.Conv1d(128, 64, 4, padding=2), nn.ReLU(), # 64@26\n",
    "            nn.Conv1d(64, 16, 3), nn.ReLU(), # 16@24\n",
    "        )\n",
    "        \n",
    "        # Image의 Convolution Block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), # 32@28x28\n",
    "            nn.Conv2d(32, 128, 5, padding=2), nn.ReLU(), # 128@28x28\n",
    "            nn.Conv2d(128, 256, 7, padding=3), nn.ReLU(), # 256@28x28\n",
    "            nn.Conv2d(256, 512, 9, padding=3), nn.ReLU(), # 512@26x26\n",
    "            nn.Conv2d(512, 256, 9, padding=3), nn.ReLU(), # 256@24x24\n",
    "            nn.Conv2d(256, 128, 7, padding=3), nn.ReLU(), # 128@24x24\n",
    "            nn.Conv2d(128, 64, 7, padding=3), nn.ReLU(), # 64@24x24\n",
    "            nn.Conv2d(64, 32, 5, padding=3), nn.ReLU(), # 32@26x26\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "            Output Size = (W + 2xP - Dx(K-1) -1) / S + 1\n",
    "\n",
    "            W: input_volume_size\n",
    "            K: kernel_size\n",
    "            D: Dilation\n",
    "            P: padding_size\n",
    "            S: strides\n",
    "        '''\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(22016, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x1, x2, label=False):\n",
    "        out = self._inference(x1, x2)\n",
    "        if label is not False:\n",
    "            loss = self.loss(out, label)\n",
    "            return (out, loss)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _inference(self, x1, x2):\n",
    "        bsz = x1.size(0)\n",
    "#         print('bsz: ', bsz)\n",
    "        x1 = self.conv1(x1)\n",
    "#         print('x1 shape: ', x1.shape) # [32, 16, 24]\n",
    "        x2 = self.conv2(x2)\n",
    "#         print('x2 shape: ', x2.shape) # [32, 32, 26, 26]\n",
    "        \n",
    "        x1 = x1.view(bsz, -1)\n",
    "#         print('x1 shape: ', x1.shape) # [32, 384]\n",
    "        x2 = x2.view(bsz, -1)\n",
    "#         print('x2 shape: ', x2.shape) # [32, 21632]\n",
    "        \n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        out = torch.nn.functional.softmax(self.out(x), dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customCNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(16, 64, kernel_size=(4,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(128, 64, kernel_size=(4,), stride=(1,), padding=(2,))\n",
       "    (7): ReLU()\n",
       "    (8): Conv1d(64, 16, kernel_size=(3,), stride=(1,))\n",
       "    (9): ReLU()\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(256, 512, kernel_size=(9, 9), stride=(1, 1), padding=(3, 3))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(512, 256, kernel_size=(9, 9), stride=(1, 1), padding=(3, 3))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(256, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(3, 3))\n",
       "    (15): ReLU()\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=22016, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = customCNN()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_letter = x_train[:32, :, :26].cuda()\n",
    "test_image = x_train[:32, :, 26:].reshape(-1, 1, 28, 28).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 26])\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(test_letter.shape)\n",
    "print(test_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932],\n",
       "        [0.0962, 0.1225, 0.0985, 0.0921, 0.0946, 0.1193, 0.1020, 0.0926, 0.0890,\n",
       "         0.0932]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_letter, test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    lr=2e-5,\n",
    "    eps=1e-8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 150/150] Avg Training Loss:  1.55 Valid Acc: 0.61\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# gradient 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "history = defaultdict(list)\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    # train 모드로 변경\n",
    "    model.train()\n",
    "    \n",
    "    # dataloader에서 batch size만큼 반복해서 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # batch를 GPU에 적용\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # batch에서 데이터 추출\n",
    "        letter, image, label = batch\n",
    "        \n",
    "        # Forward Propagation 수행\n",
    "        outputs = model(letter, image, label)\n",
    "        \n",
    "        loss = outputs[1]\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward Propagation 수행\n",
    "        loss.backward()\n",
    "        history[\"train_loss\"].append(loss.item())\n",
    "        \n",
    "        # 정확도 계산\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "        label = label.to(\"cpu\").numpy()\n",
    "        tmp_train_accuracy = flat_accuracy(logits, label)\n",
    "        history[\"train_acc\"].append(tmp_train_accuracy)\n",
    "        \n",
    "        # Gradient Cleeping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # gradient를 통해 weight update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gradient 초기화\n",
    "        model.zero_grad()\n",
    "        \n",
    "    # average loss\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # eval 모드로 변경\n",
    "    model.eval()\n",
    "    \n",
    "    # 변수 초기화\n",
    "    eval_loss, eval_accuracy, nb_eval_steps, nb_eval_examples = 0, 0, 0, 0\n",
    "    \n",
    "    # dataloader에서 batch만큼 반복해서 가져옴\n",
    "    for batch in valid_dataloader:\n",
    "        \n",
    "        # batch를 GPU에 적용\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # batch에서 데이터 추출\n",
    "        letter, image, label = batch\n",
    "        \n",
    "        # gradient 계산 안함\n",
    "        with torch.no_grad():\n",
    "            # Forward Propagation 수행\n",
    "            outputs = model(letter, image, label)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        history[\"eval_loss\"].append(outputs[1].item())\n",
    "        \n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label = label.to(\"cpu\").numpy()\n",
    "        \n",
    "        # 출력 logit과 label을 비교하여 정확도 계산\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label)\n",
    "        history[\"eval_acc\"].append(tmp_eval_accuracy)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    s = f\"\\r[Epoch {epoch_i+1}/{epochs}]\"\n",
    "    s += f\" Avg Training Loss: {avg_train_loss: .2f}\"\n",
    "    s += \" Valid Acc: {0:.2f}\".format(eval_accuracy / nb_eval_steps)\n",
    "    print(s, end=\"\")\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model/emnist_model3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customCNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(16, 64, kernel_size=(4,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(128, 64, kernel_size=(4,), stride=(1,), padding=(2,))\n",
       "    (7): ReLU()\n",
       "    (8): Conv1d(64, 16, kernel_size=(3,), stride=(1,))\n",
       "    (9): ReLU()\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(128, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(256, 512, kernel_size=(9, 9), stride=(1, 1), padding=(3, 3))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(512, 256, kernel_size=(9, 9), stride=(1, 1), padding=(3, 3))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(256, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(128, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(3, 3))\n",
       "    (15): ReLU()\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=22016, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = customCNN()\n",
    "model.load_state_dict(torch.load(\"./model/emnist_model3.pt\"))\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.concatenate(\n",
    "    [\n",
    "        pd.get_dummies(test[\"letter\"]).values.reshape(-1, 1, 26),\n",
    "        (test[[str(i) for i in range(784)]] / 255.).values.reshape(-1, 1, 784)\n",
    "    ],\n",
    "    axis=2\n",
    ")\n",
    "x_test = torch.Tensor(x_test)\n",
    "\n",
    "x1 = x_test[:, :, :26].cuda()\n",
    "x2 = x_test[:, :, 26:].reshape(-1, 1, 28, 28).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(x1, x2)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for batch in test_dataloader:\n",
    "    input1, input2 = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input1, input2)\n",
    "    y_pred.append(torch.argmax(outputs, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"digit\"] = torch.cat(y_pred).detach().cpu().numpy()\n",
    "submission.to_csv(\"./result/submission3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
