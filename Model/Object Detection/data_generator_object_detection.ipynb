{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "data_generator_object_detection.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ioWXP7qSOWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a80492ac-1af6-4a11-9a1a-5c13f08593e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raCoKQmDSArB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "from PIL import Image\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numba\n",
        "import ast\n",
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "from numba import jit\n",
        "from typing import List, Union, Tuple"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L1UQMxSSArE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5ded7a20-617a-44a3-e203-34afeaa78154"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/drive/My Drive/Data_Set/data.csv\")\n",
        "train_df"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class_id</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000.jpg</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>41.558442</td>\n",
              "      <td>597.922078</td>\n",
              "      <td>571.428571</td>\n",
              "      <td>squid_peanut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000.jpg</td>\n",
              "      <td>558.441558</td>\n",
              "      <td>137.142857</td>\n",
              "      <td>740.259740</td>\n",
              "      <td>344.935065</td>\n",
              "      <td>cidar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00000.jpg</td>\n",
              "      <td>715.324675</td>\n",
              "      <td>69.610390</td>\n",
              "      <td>906.493506</td>\n",
              "      <td>395.844156</td>\n",
              "      <td>diget_ori</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00000.jpg</td>\n",
              "      <td>902.337662</td>\n",
              "      <td>21.818182</td>\n",
              "      <td>1082.077922</td>\n",
              "      <td>362.597403</td>\n",
              "      <td>welchs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00000.jpg</td>\n",
              "      <td>270.649351</td>\n",
              "      <td>431.168831</td>\n",
              "      <td>505.454545</td>\n",
              "      <td>646.233766</td>\n",
              "      <td>diget_choco</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17751</th>\n",
              "      <td>02002.jpg</td>\n",
              "      <td>796.560311</td>\n",
              "      <td>99.585062</td>\n",
              "      <td>1181.058366</td>\n",
              "      <td>228.049792</td>\n",
              "      <td>vita500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17752</th>\n",
              "      <td>02002.jpg</td>\n",
              "      <td>453.898833</td>\n",
              "      <td>264.896266</td>\n",
              "      <td>780.622568</td>\n",
              "      <td>468.049792</td>\n",
              "      <td>diget_ori</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17753</th>\n",
              "      <td>02002.jpg</td>\n",
              "      <td>772.653697</td>\n",
              "      <td>231.037344</td>\n",
              "      <td>1047.579767</td>\n",
              "      <td>657.261411</td>\n",
              "      <td>jjolbyung_noodle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17754</th>\n",
              "      <td>02002.jpg</td>\n",
              "      <td>1039.610895</td>\n",
              "      <td>213.112033</td>\n",
              "      <td>1239.828794</td>\n",
              "      <td>879.336100</td>\n",
              "      <td>zec</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17755</th>\n",
              "      <td>02002.jpg</td>\n",
              "      <td>374.210117</td>\n",
              "      <td>585.560166</td>\n",
              "      <td>1007.735409</td>\n",
              "      <td>955.020747</td>\n",
              "      <td>squid_peanut</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17756 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        class_id         xmin  ...        ymax             label\n",
              "0      00000.jpg     0.000000  ...  571.428571      squid_peanut\n",
              "1      00000.jpg   558.441558  ...  344.935065             cidar\n",
              "2      00000.jpg   715.324675  ...  395.844156         diget_ori\n",
              "3      00000.jpg   902.337662  ...  362.597403            welchs\n",
              "4      00000.jpg   270.649351  ...  646.233766       diget_choco\n",
              "...          ...          ...  ...         ...               ...\n",
              "17751  02002.jpg   796.560311  ...  228.049792           vita500\n",
              "17752  02002.jpg   453.898833  ...  468.049792         diget_ori\n",
              "17753  02002.jpg   772.653697  ...  657.261411  jjolbyung_noodle\n",
              "17754  02002.jpg  1039.610895  ...  879.336100               zec\n",
              "17755  02002.jpg   374.210117  ...  955.020747      squid_peanut\n",
              "\n",
              "[17756 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qXqRTLUSArH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_ids = train_df['class_id'].unique()\n",
        "valid_ids = image_ids[-655:]\n",
        "train_ids = image_ids[:-655]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDz2Xr31SArJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_df = train_df[train_df['class_id'].isin(valid_ids)]  \n",
        "train_df = train_df[train_df['class_id'].isin(train_ids)]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjBaPUFVSArL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class customDataSet(object):\n",
        "    def __init__(self, dataframe, image_dir, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_ids = dataframe['class_id'].unique()\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.image_ids.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image_id = self.image_ids[index]\n",
        "        records = self.df[self.df['class_id'] == image_id]\n",
        "        image_name = image_id.split(\".\")[0]\n",
        "\n",
        "        image = cv2.imread(f'{self.image_dir}/{image_name}.jpg', cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "        \n",
        "        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        area = torch.as_tensor(area, dtype=torch.float32)\n",
        "\n",
        "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
        "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "        target['area'] = area\n",
        "        target['iscrowd'] = iscrowd\n",
        "        \n",
        "        if self.transforms:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "            \n",
        "            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
        "        \n",
        "        return image, target, image_id"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmz3TbFXSArM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Albumentations\n",
        "def get_train_transform():\n",
        "    return A.Compose([\n",
        "#         A.Flip(0.5),\n",
        "#         ToTensorV2(p=1.0)\n",
        "        ToTensor(),\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "\n",
        "def get_valid_transform():\n",
        "    return A.Compose([\n",
        "#         ToTensorV2(p=1.0)\n",
        "        ToTensor(),\n",
        "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxBTMRfYSArP",
        "colab_type": "text"
      },
      "source": [
        "## 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ2fpXPuSArQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVUbGMO0SArR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 23 # 22 class + background\n",
        "\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infEU4y3SArT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWOCcf6iSArV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "DIR_TRAIN = \"/content/drive/My Drive/Data_Set/image\"\n",
        "train_dataset = customDataSet(train_df, DIR_TRAIN, get_train_transform())\n",
        "valid_dataset = customDataSet(valid_df, DIR_TRAIN, get_valid_transform())\n",
        "\n",
        "\n",
        "# split the dataset in train and test set\n",
        "indices = torch.randperm(len(train_dataset)).tolist()\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "valid_data_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReeiAhplSArW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "29cac4af-ba3d-4ba7-da24-fef0acca9c49"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.7098, 0.7059, 0.7059,  ..., 0.7059, 0.7059, 0.7059],\n",
              "          [0.7098, 0.7059, 0.7059,  ..., 0.7059, 0.7059, 0.7059],\n",
              "          [0.7098, 0.7059, 0.7059,  ..., 0.7059, 0.7059, 0.7059],\n",
              "          ...,\n",
              "          [0.7765, 0.7765, 0.7765,  ..., 0.0078, 0.0078, 0.0078],\n",
              "          [0.7765, 0.7765, 0.7765,  ..., 0.0078, 0.0078, 0.0078],\n",
              "          [0.7765, 0.7765, 0.7765,  ..., 0.0078, 0.0078, 0.0078]],\n",
              " \n",
              "         [[0.7569, 0.7529, 0.7529,  ..., 0.7882, 0.7882, 0.7882],\n",
              "          [0.7569, 0.7529, 0.7529,  ..., 0.7882, 0.7882, 0.7882],\n",
              "          [0.7569, 0.7529, 0.7529,  ..., 0.7882, 0.7882, 0.7882],\n",
              "          ...,\n",
              "          [0.8667, 0.8667, 0.8667,  ..., 0.0235, 0.0235, 0.0235],\n",
              "          [0.8667, 0.8667, 0.8667,  ..., 0.0235, 0.0235, 0.0235],\n",
              "          [0.8667, 0.8667, 0.8667,  ..., 0.0235, 0.0235, 0.0235]],\n",
              " \n",
              "         [[0.8510, 0.8471, 0.8471,  ..., 0.8706, 0.8706, 0.8706],\n",
              "          [0.8510, 0.8471, 0.8471,  ..., 0.8706, 0.8706, 0.8706],\n",
              "          [0.8510, 0.8471, 0.8471,  ..., 0.8706, 0.8706, 0.8706],\n",
              "          ...,\n",
              "          [0.9922, 0.9922, 0.9882,  ..., 0.0196, 0.0196, 0.0196],\n",
              "          [0.9922, 0.9922, 0.9882,  ..., 0.0196, 0.0196, 0.0196],\n",
              "          [0.9922, 0.9922, 0.9882,  ..., 0.0196, 0.0196, 0.0196]]]),\n",
              " {'area': tensor([316821.0625,  37780.4023,  62365.7266,  61251.7461,  50498.3633,\n",
              "           48972.0352,  26602.7988,  93894.0156, 167097.3125, 101628.2031]),\n",
              "  'boxes': tensor([[   0.0000,   41.5584,  597.9221,  571.4286],\n",
              "          [ 558.4416,  137.1429,  740.2597,  344.9351],\n",
              "          [ 715.3247,   69.6104,  906.4935,  395.8442],\n",
              "          [ 902.3377,   21.8182, 1082.0779,  362.5974],\n",
              "          [ 270.6494,  431.1688,  505.4545,  646.2338],\n",
              "          [ 550.1299,  408.3117,  772.4675,  628.5714],\n",
              "          [ 521.0390,  620.2597,  686.2338,  781.2987],\n",
              "          [ 672.7273,  608.8312,  944.9351,  953.7662],\n",
              "          [ 884.6753,  343.8961, 1196.3636,  880.0000],\n",
              "          [ 150.1299,  627.5325,  458.7013,  956.8831]], dtype=torch.float64),\n",
              "  'image_id': tensor([0]),\n",
              "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])},\n",
              " '00000.jpg')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAmhVqibSArY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit(nopython=True)\n",
        "def calculate_iou(gt, pr, form='pascal_voc') -> float:\n",
        "    \"\"\"Calculates the Intersection over Union.\n",
        "\n",
        "    Args:\n",
        "        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n",
        "        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n",
        "        form: (str) gt/pred coordinates format\n",
        "            - pascal_voc: [xmin, ymin, xmax, ymax]\n",
        "            - coco: [xmin, ymin, w, h]\n",
        "    Returns:\n",
        "        (float) Intersection over union (0.0 <= iou <= 1.0)\n",
        "    \"\"\"\n",
        "    if form == 'coco':\n",
        "        gt = gt.copy()\n",
        "        pr = pr.copy()\n",
        "\n",
        "        gt[2] = gt[0] + gt[2]\n",
        "        gt[3] = gt[1] + gt[3]\n",
        "        pr[2] = pr[0] + pr[2]\n",
        "        pr[3] = pr[1] + pr[3]\n",
        "\n",
        "    # Calculate overlap area\n",
        "    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n",
        "    \n",
        "    if dx < 0:\n",
        "        return 0.0\n",
        "    \n",
        "    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n",
        "\n",
        "    if dy < 0:\n",
        "        return 0.0\n",
        "\n",
        "    overlap_area = dx * dy\n",
        "\n",
        "    # Calculate union area\n",
        "    union_area = (\n",
        "            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n",
        "            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n",
        "            overlap_area\n",
        "    )\n",
        "\n",
        "    return overlap_area / union_area"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "And2_xEzSAra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit(nopython=True)\n",
        "def find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n",
        "    \"\"\"Returns the index of the 'best match' between the\n",
        "    ground-truth boxes and the prediction. The 'best match'\n",
        "    is the highest IoU. (0.0 IoUs are ignored).\n",
        "\n",
        "    Args:\n",
        "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
        "        pred: (List[Union[int, float]]) Coordinates of the predicted box\n",
        "        pred_idx: (int) Index of the current predicted box\n",
        "        threshold: (float) Threshold\n",
        "        form: (str) Format of the coordinates\n",
        "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
        "\n",
        "    Return:\n",
        "        (int) Index of the best match GT box (-1 if no match above threshold)\n",
        "    \"\"\"\n",
        "    best_match_iou = -np.inf\n",
        "    best_match_idx = -1\n",
        "\n",
        "    for gt_idx in range(len(gts)):\n",
        "        \n",
        "        if gts[gt_idx][0] < 0:\n",
        "            # Already matched GT-box\n",
        "            continue\n",
        "        \n",
        "        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n",
        "\n",
        "        if iou < 0:\n",
        "            iou = calculate_iou(gts[gt_idx], pred, form=form)\n",
        "            \n",
        "            if ious is not None:\n",
        "                ious[gt_idx][pred_idx] = iou\n",
        "\n",
        "        if iou < threshold:\n",
        "            continue\n",
        "\n",
        "        if iou > best_match_iou:\n",
        "            best_match_iou = iou\n",
        "            best_match_idx = gt_idx\n",
        "\n",
        "    return best_match_idx\n",
        "\n",
        "@jit(nopython=True)\n",
        "def calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n",
        "    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n",
        "\n",
        "    Args:\n",
        "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
        "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
        "               sorted by confidence value (descending)\n",
        "        threshold: (float) Threshold\n",
        "        form: (str) Format of the coordinates\n",
        "        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n",
        "\n",
        "    Return:\n",
        "        (float) Precision\n",
        "    \"\"\"\n",
        "    n = len(preds)\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    \n",
        "    # for pred_idx, pred in enumerate(preds_sorted):\n",
        "    for pred_idx in range(n):\n",
        "\n",
        "        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx, threshold=threshold, form=form, ious=ious)\n",
        "\n",
        "        if best_match_gt_idx >= 0:\n",
        "            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n",
        "            tp += 1\n",
        "            # Remove the matched GT box\n",
        "            gts[best_match_gt_idx] = -1\n",
        "\n",
        "        else:\n",
        "            # No match\n",
        "            # False positive: indicates a predicted box had no associated gt box.\n",
        "            fp += 1\n",
        "\n",
        "    # False negative: indicates a gt box had no associated predicted box.\n",
        "    fn = (gts.sum(axis=1) > 0).sum()\n",
        "\n",
        "    return tp / (tp + fp + fn)\n",
        "\n",
        "\n",
        "@jit(nopython=True)\n",
        "def calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n",
        "    \"\"\"Calculates image precision.\n",
        "\n",
        "    Args:\n",
        "        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n",
        "        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n",
        "               sorted by confidence value (descending)\n",
        "        thresholds: (float) Different thresholds\n",
        "        form: (str) Format of the coordinates\n",
        "\n",
        "    Return:\n",
        "        (float) Precision\n",
        "    \"\"\"\n",
        "    n_threshold = len(thresholds)\n",
        "    image_precision = 0.0\n",
        "    \n",
        "    ious = np.ones((len(gts), len(preds))) * -1\n",
        "    # ious = None\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold, form=form, ious=ious)\n",
        "        image_precision += precision_at_threshold / n_threshold\n",
        "\n",
        "    return image_precision"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzx8NpwhTACD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "935ac315-8648-4f8f-af68-b8c5f9700b38"
      },
      "source": [
        "!pip install ensemble_boxes"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ensemble_boxes in /usr/local/lib/python3.6/dist-packages (1.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ensemble_boxes) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ensemble_boxes) (1.0.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from ensemble_boxes) (0.48.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->ensemble_boxes) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ensemble_boxes) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->ensemble_boxes) (50.3.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->ensemble_boxes) (0.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->ensemble_boxes) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "859_hErqSArc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = [model]\n",
        "from ensemble_boxes import *\n",
        "\n",
        "def make_ensemble_predictions(images):\n",
        "    images = list(image.to(device) for image in images)    \n",
        "    result = []\n",
        "    for net in models:\n",
        "        net.eval()\n",
        "        outputs = net(images)\n",
        "        result.append(outputs)\n",
        "    return result\n",
        "\n",
        "def run_wbf(predictions, image_index, image_size=1280, iou_thr=0.55, skip_box_thr=0.5, weights=None):\n",
        "    boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n",
        "    scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n",
        "    labels = [np.ones(prediction[image_index]['scores'].shape[0]) for prediction in predictions]\n",
        "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
        "    boxes = boxes*(image_size-1)\n",
        "    return boxes, scores, labels"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK9lVNaSArd",
        "colab_type": "text"
      },
      "source": [
        "## 훈련"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTqtwSs4SAre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruMMxl7pSArf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to(device)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "num_epochs = 10"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tboWE5gBSArh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "outputId": "1acd1e64-cda9-4399-a0df-15bfe3717611"
      },
      "source": [
        "loss_hist = Averager()\n",
        "best_val = None\n",
        "es_patience = 2\n",
        "patience = es_patience\n",
        "model_path = '/content/drive/My Drive/Model/lotte_model_object_detection.pt'\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    itr = 1\n",
        "    loss_hist.reset()\n",
        "    model.train()\n",
        "    for images, targets, image_ids in train_data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if k =='labels' else v.float().to(device) for k, v in t.items()} for t in targets]#[{k: v.double().to(device) if k =='boxes' else v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss_value = losses.item()\n",
        "\n",
        "        loss_hist.send(loss_value)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if itr % 50 == 0:\n",
        "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
        "\n",
        "        itr += 1\n",
        "    \n",
        "    # update the learning rate\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    #At every epoch we will also calculate the validation IOU\n",
        "    validation_image_precisions = []\n",
        "    iou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n",
        "    model.eval()\n",
        "    for images, targets,imageids in valid_data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if k =='labels' else v.float().to(device) for k, v in t.items()} for t in targets]\n",
        "        #outputs = model(images) \n",
        "        \n",
        "        predictions = make_ensemble_predictions(images)\n",
        "   \n",
        "        for i, image in enumerate(images):\n",
        "            boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
        "            boxes = boxes.astype(np.int32).clip(min=0, max=1023)\n",
        "            \n",
        "            preds = boxes#outputs[i]['boxes'].data.cpu().numpy()\n",
        "            #scores = outputs[i]['scores'].data.cpu().numpy()\n",
        "            preds_sorted_idx = np.argsort(scores)[::-1]\n",
        "            preds_sorted = preds[preds_sorted_idx]\n",
        "            gt_boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
        "            image_precision = calculate_image_precision(preds_sorted, gt_boxes, thresholds=iou_thresholds, form='coco')\n",
        "\n",
        "            validation_image_precisions.append(image_precision)\n",
        "    \n",
        "    val_iou = np.mean(validation_image_precisions)\n",
        "    print(f\"Epoch #{epoch+1} loss: {loss_hist.value}\",\"Validation IOU: {0:.4f}\".format(val_iou),\"Time taken :\",str(datetime.timedelta(seconds=time.time() - start_time))[:7])\n",
        "    \n",
        "    if not best_val:\n",
        "        best_val = val_iou  # So any validation roc_auc we have is the best one for now\n",
        "        print(\"Saving model\")\n",
        "        torch.save(model, model_path)  # Saving the model\n",
        "        #continue\n",
        "    if val_iou >= best_val:\n",
        "        print(\"Saving model as IOU is increased from\",best_val,\"to\",val_iou)\n",
        "        best_val = val_iou\n",
        "        patience = es_patience  # Resetting patience since we have new best validation accuracy\n",
        "        torch.save(model, model_path)  # Saving current best model torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn.pth')\n",
        "    else:\n",
        "        patience -= 1\n",
        "        if patience == 0:\n",
        "            print('Early stopping. Best Validation IOU: {:.3f}'.format(best_val))\n",
        "            break"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration #50 loss: 0.4001063406467438\n",
            "Iteration #100 loss: 0.5076561570167542\n",
            "Iteration #150 loss: 0.40842247009277344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ensemble_boxes/ensemble_boxes_wbf.py:73: UserWarning: X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.\n",
            "  warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #1 loss: 0.4576162611239055 Validation IOU: 0.8307 Time taken : 0:04:28\n",
            "Saving model\n",
            "Saving model as IOU is increased from 0.8306884402248121 to 0.8306884402248121\n",
            "Iteration #50 loss: 0.27906206250190735\n",
            "Iteration #100 loss: 0.35777297616004944\n",
            "Iteration #150 loss: 0.3315061330795288\n",
            "Epoch #2 loss: 0.34677309353323377 Validation IOU: 0.8572 Time taken : 0:04:28\n",
            "Saving model as IOU is increased from 0.8306884402248121 to 0.8572227355624302\n",
            "Iteration #50 loss: 0.2964670956134796\n",
            "Iteration #100 loss: 0.3512054681777954\n",
            "Iteration #150 loss: 0.335936039686203\n",
            "Epoch #3 loss: 0.324726652993253 Validation IOU: 0.8723 Time taken : 0:04:28\n",
            "Saving model as IOU is increased from 0.8572227355624302 to 0.8723022460999561\n",
            "Iteration #50 loss: 0.2929338216781616\n",
            "Iteration #100 loss: 0.3491532802581787\n",
            "Iteration #150 loss: 0.32173508405685425\n",
            "Epoch #4 loss: 0.31340814897647273 Validation IOU: 0.8830 Time taken : 0:04:28\n",
            "Saving model as IOU is increased from 0.8723022460999561 to 0.883016423073675\n",
            "Iteration #50 loss: 0.2507115304470062\n",
            "Iteration #100 loss: 0.3329147696495056\n",
            "Iteration #150 loss: 0.33244746923446655\n",
            "Epoch #5 loss: 0.3036567576714521 Validation IOU: 0.8860 Time taken : 0:04:28\n",
            "Saving model as IOU is increased from 0.883016423073675 to 0.8859605527353619\n",
            "Iteration #50 loss: 0.2552354037761688\n",
            "Iteration #100 loss: 0.34389257431030273\n",
            "Iteration #150 loss: 0.33110323548316956\n",
            "Epoch #6 loss: 0.29992678444061055 Validation IOU: 0.8885 Time taken : 0:04:28\n",
            "Saving model as IOU is increased from 0.8859605527353619 to 0.8884620157902601\n",
            "Iteration #50 loss: 0.24859437346458435\n",
            "Iteration #100 loss: 0.3282219171524048\n",
            "Iteration #150 loss: 0.3258613049983978\n",
            "Epoch #7 loss: 0.2986473230391564 Validation IOU: 0.8905 Time taken : 0:04:28\n",
            "Saving model as IOU is increased from 0.8884620157902601 to 0.8904963378436661\n",
            "Iteration #50 loss: 0.24283437430858612\n",
            "Iteration #100 loss: 0.338398814201355\n",
            "Iteration #150 loss: 0.3056492805480957\n",
            "Epoch #8 loss: 0.29820478685508817 Validation IOU: 0.8896 Time taken : 0:04:28\n",
            "Iteration #50 loss: 0.24291066825389862\n",
            "Iteration #100 loss: 0.3364482820034027\n",
            "Iteration #150 loss: 0.31665709614753723\n",
            "Epoch #9 loss: 0.2992362609276405 Validation IOU: 0.8902 Time taken : 0:04:28\n",
            "Early stopping. Best Validation IOU: 0.890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHLHER9g2Edl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_model = torch.load(model_path)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl3tJQnM2Kcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIR_TEST = f'{DIR_INPUT}/test'\n",
        "test_df = pd.DataFrame()\n",
        "test_df['image_id'] = np.array([path.split('/')[-1][:-4] for path in glob(f'{DIR_TEST}/*.jpg')])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}