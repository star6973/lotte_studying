{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random as rand\n",
    "from random import *\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Data_Set/Labeld_Crop_Data/\"\n",
    "trDsets = {x: dset.ImageFolder(os.path.join(data_dir, x), train_transforms[x]) for x in ['train', 'val']}\n",
    "trLoaders = {x: torch.utils.data.DataLoader(trDsets[x], batch_size=64, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "teDsets = dset.ImageFolder(os.path.join(data_dir, 'test'), transform=test_transforms)\n",
    "teLoaders = torch.utils.data.DataLoader(teDsets, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trDsets_sizes = {x: len(trDsets[x]) for x in ['train', 'val']}\n",
    "class_names = trDsets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=22, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnext50_32x4d(pretrained=False, num_classes=22)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor=0.1, patience=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=8, device=device):\n",
    "    \n",
    "    global_info = []\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    early_stopping = EarlyStopping(patience=11, verbose=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        local_info = []\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "                if epoch > 0:\n",
    "                    scheduler.step(val_loss)\n",
    "                    \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in trLoaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / trDsets_sizes[phase]\n",
    "            if phase == 'val':\n",
    "                val_loss = running_loss / trDsets_sizes['val']\n",
    "            epoch_acc = running_corrects.double() / trDsets_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                local_info.append(epoch_loss)\n",
    "                ea = epoch_acc.cpu().numpy()\n",
    "                local_info.append(ea)\n",
    "            else:\n",
    "                local_info.append(epoch_loss)\n",
    "                ea = epoch_acc.cpu().numpy()\n",
    "                local_info.append(ea)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        lr_get = get_lr(optimizer)\n",
    "        print(\"Current learning rate : {:.8f}\".format(lr_get))\n",
    "        global_info.append(local_info)\n",
    "        \n",
    "        if phase =='val':\n",
    "            early_stopping(epoch_loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.5219 Acc: 0.2323\n",
      "val Loss: 1.7672 Acc: 0.4471\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (inf --> 1.767185).  Saving model ...\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 1.3464 Acc: 0.5762\n",
      "val Loss: 0.9120 Acc: 0.7048\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (1.767185 --> 0.912002).  Saving model ...\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.7442 Acc: 0.7634\n",
      "val Loss: 0.5087 Acc: 0.8466\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.912002 --> 0.508685).  Saving model ...\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.4799 Acc: 0.8489\n",
      "val Loss: 0.3202 Acc: 0.9073\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.508685 --> 0.320238).  Saving model ...\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.3219 Acc: 0.9019\n",
      "val Loss: 0.2485 Acc: 0.9281\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.320238 --> 0.248489).  Saving model ...\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.2604 Acc: 0.9232\n",
      "val Loss: 0.1833 Acc: 0.9477\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.248489 --> 0.183266).  Saving model ...\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.9378\n",
      "val Loss: 0.1951 Acc: 0.9425\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.1641 Acc: 0.9536\n",
      "val Loss: 0.1518 Acc: 0.9537\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.183266 --> 0.151839).  Saving model ...\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.1397 Acc: 0.9586\n",
      "val Loss: 0.1266 Acc: 0.9656\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.151839 --> 0.126583).  Saving model ...\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.1229 Acc: 0.9639\n",
      "val Loss: 0.0979 Acc: 0.9716\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.126583 --> 0.097936).  Saving model ...\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.0972 Acc: 0.9716\n",
      "val Loss: 0.1005 Acc: 0.9756\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.1022 Acc: 0.9700\n",
      "val Loss: 0.0937 Acc: 0.9772\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.097936 --> 0.093728).  Saving model ...\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0928 Acc: 0.9722\n",
      "val Loss: 0.1035 Acc: 0.9724\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.9780\n",
      "val Loss: 0.0834 Acc: 0.9788\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.093728 --> 0.083361).  Saving model ...\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0663 Acc: 0.9821\n",
      "val Loss: 0.0829 Acc: 0.9824\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.083361 --> 0.082931).  Saving model ...\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0703 Acc: 0.9783\n",
      "val Loss: 0.0972 Acc: 0.9768\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0629 Acc: 0.9810\n",
      "val Loss: 0.0869 Acc: 0.9808\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0598 Acc: 0.9802\n",
      "val Loss: 0.0916 Acc: 0.9776\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 3 out of 11\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0555 Acc: 0.9837\n",
      "val Loss: 0.0788 Acc: 0.9824\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.082931 --> 0.078806).  Saving model ...\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0450 Acc: 0.9867\n",
      "val Loss: 0.0868 Acc: 0.9792\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0539 Acc: 0.9828\n",
      "val Loss: 0.0834 Acc: 0.9828\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0436 Acc: 0.9877\n",
      "val Loss: 0.0775 Acc: 0.9844\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.078806 --> 0.077476).  Saving model ...\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0496 Acc: 0.9855\n",
      "val Loss: 0.0802 Acc: 0.9832\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0431 Acc: 0.9872\n",
      "val Loss: 0.0880 Acc: 0.9812\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0507 Acc: 0.9834\n",
      "val Loss: 0.0771 Acc: 0.9852\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.077476 --> 0.077105).  Saving model ...\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0504 Acc: 0.9841\n",
      "val Loss: 0.0742 Acc: 0.9832\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.077105 --> 0.074214).  Saving model ...\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0389 Acc: 0.9888\n",
      "val Loss: 0.0791 Acc: 0.9816\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0377 Acc: 0.9872\n",
      "val Loss: 0.0703 Acc: 0.9844\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.074214 --> 0.070332).  Saving model ...\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0286 Acc: 0.9920\n",
      "val Loss: 0.0820 Acc: 0.9816\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0533 Acc: 0.9833\n",
      "val Loss: 0.0847 Acc: 0.9820\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0342 Acc: 0.9889\n",
      "val Loss: 0.0812 Acc: 0.9832\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 3 out of 11\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0399 Acc: 0.9868\n",
      "val Loss: 0.0711 Acc: 0.9844\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 4 out of 11\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0450 Acc: 0.9859\n",
      "val Loss: 0.0701 Acc: 0.9856\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.070332 --> 0.070146).  Saving model ...\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0333 Acc: 0.9890\n",
      "val Loss: 0.0704 Acc: 0.9864\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0273 Acc: 0.9908\n",
      "val Loss: 0.0831 Acc: 0.9808\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0312 Acc: 0.9899\n",
      "val Loss: 0.0741 Acc: 0.9860\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 3 out of 11\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0316 Acc: 0.9897\n",
      "val Loss: 0.0662 Acc: 0.9868\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.070146 --> 0.066210).  Saving model ...\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0392 Acc: 0.9900\n",
      "val Loss: 0.0643 Acc: 0.9896\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.066210 --> 0.064286).  Saving model ...\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0259 Acc: 0.9927\n",
      "val Loss: 0.0701 Acc: 0.9868\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0349 Acc: 0.9895\n",
      "val Loss: 0.0706 Acc: 0.9852\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0363 Acc: 0.9889\n",
      "val Loss: 0.0827 Acc: 0.9824\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 3 out of 11\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0323 Acc: 0.9891\n",
      "val Loss: 0.0748 Acc: 0.9868\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 4 out of 11\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0216 Acc: 0.9931\n",
      "val Loss: 0.0782 Acc: 0.9856\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 5 out of 11\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0243 Acc: 0.9928\n",
      "val Loss: 0.0707 Acc: 0.9880\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 6 out of 11\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0161 Acc: 0.9949\n",
      "val Loss: 0.0726 Acc: 0.9888\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 7 out of 11\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0160 Acc: 0.9950\n",
      "val Loss: 0.0699 Acc: 0.9872\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 8 out of 11\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0273 Acc: 0.9929\n",
      "val Loss: 0.0713 Acc: 0.9852\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 9 out of 11\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0228 Acc: 0.9935\n",
      "val Loss: 0.0744 Acc: 0.9848\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 10 out of 11\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0383 Acc: 0.9878\n",
      "val Loss: 0.0694 Acc: 0.9880\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 11 out of 11\n",
      "Early stopping\n",
      "Training complete in 53m 17s\n",
      "Best val Acc: 0.989612\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_ft = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, 'lotte_model_resnext.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel = torch.load('lotte_model_resnext.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test images: 98.70 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in teLoaders:\n",
    "        images, labels = data\n",
    "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
    "        \n",
    "        outputs = testModel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'ID_gum', 1: 'buttering', 2: 'couque_coffee', 3: 'chocopie', 4: 'cidar', 5: 'couque_white', 6: 'coke', 7: 'diget_ori', 8: 'diget_choco', 9: 'gumi_gumi', 10: 'homerunball', 11: 'jjolbyung_noodle', 12: 'juicyfresh', 13: 'jjolbyung_ori', 14: 'spearmint', 15: 'squid_peanut', 16: 'samdasu', 17: 'tuna', 18: 'toreta', 19: 'vita500', 20: 'welchs', 21: 'zec'}\n"
     ]
    }
   ],
   "source": [
    "tag_classes = ['ID_gum', 'buttering', 'couque_coffee', 'chocopie', 'cidar', 'couque_white', 'coke', 'diget_ori', 'diget_choco', 'gumi_gumi', 'homerunball', 'jjolbyung_noodle', 'juicyfresh', 'jjolbyung_ori', 'spearmint', 'squid_peanut', 'samdasu', 'tuna', 'toreta', 'vita500', 'welchs', 'zec']\n",
    "tag_dict = dict()\n",
    "for i, label in enumerate(tag_classes):\n",
    "    tag_dict[i] = label\n",
    "\n",
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ID_gum : 98.28 %\n",
      "Accuracy of buttering : 100.00 %\n",
      "Accuracy of homerunball : 100.00 %\n",
      "Accuracy of jjolbyung_noodle : 100.00 %\n",
      "Accuracy of juicyfresh : 100.00 %\n",
      "Accuracy of jjolbyung_ori : 100.00 %\n",
      "Accuracy of spearmint : 97.70 %\n",
      "Accuracy of squid_peanut : 100.00 %\n",
      "Accuracy of samdasu : 99.51 %\n",
      "Accuracy of  tuna : 98.99 %\n",
      "Accuracy of toreta : 98.72 %\n",
      "Accuracy of vita500 : 99.47 %\n",
      "Accuracy of couque_coffee : 99.57 %\n",
      "Accuracy of welchs : 100.00 %\n",
      "Accuracy of   zec : 98.65 %\n",
      "Accuracy of chocopie : 98.60 %\n",
      "Accuracy of cidar : 99.60 %\n",
      "Accuracy of couque_white : 97.98 %\n",
      "Accuracy of  coke : 98.45 %\n",
      "Accuracy of diget_ori : 96.99 %\n",
      "Accuracy of diget_choco : 92.15 %\n",
      "Accuracy of gumi_gumi : 100.00 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0 for i in range(len(class_names)))\n",
    "class_total = list(0 for i in range(len(class_names)))\n",
    "with torch.no_grad():\n",
    "    for data in teLoaders:\n",
    "        images, labels = data\n",
    "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
    "        \n",
    "        outputs = testModel(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "\n",
    "        for i in range(c.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[int(label.item())] += c[i].item()\n",
    "            class_total[int(label.item())] += 1\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print('Accuracy of %5s : %.2f %%' % (tag_dict[int(class_names[i])], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomChoice([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=(0.5, 1.5)),\n",
    "            transforms.ColorJitter(contrast=(0.5, 1.5)),\n",
    "        ]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Data_Set/Labeld_Crop_Data/\"\n",
    "trDsets = {x: dset.ImageFolder(os.path.join(data_dir, x), train_transforms[x]) for x in ['train', 'val']}\n",
    "trLoaders = {x: torch.utils.data.DataLoader(trDsets[x], batch_size=64, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "teDsets = dset.ImageFolder(os.path.join(data_dir, 'test'), transform=test_transforms)\n",
    "teLoaders = torch.utils.data.DataLoader(teDsets, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "trDsets_sizes = {x: len(trDsets[x]) for x in ['train', 'val']}\n",
    "class_names = trDsets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=22, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnext50_32x4d(pretrained=False, num_classes=22)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor=0.1, patience=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 2.7083 Acc: 0.1748\n",
      "val Loss: 1.9544 Acc: 0.3983\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (inf --> 1.954406).  Saving model ...\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 1.6092 Acc: 0.4673\n",
      "val Loss: 0.9661 Acc: 0.6776\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (1.954406 --> 0.966142).  Saving model ...\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.9558 Acc: 0.6854\n",
      "val Loss: 0.5845 Acc: 0.8182\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.966142 --> 0.584451).  Saving model ...\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.6528 Acc: 0.7969\n",
      "val Loss: 0.3995 Acc: 0.8622\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.584451 --> 0.399544).  Saving model ...\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.4981 Acc: 0.8475\n",
      "val Loss: 0.2955 Acc: 0.9033\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.399544 --> 0.295455).  Saving model ...\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.4082 Acc: 0.8715\n",
      "val Loss: 0.2631 Acc: 0.9169\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.295455 --> 0.263107).  Saving model ...\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.3473 Acc: 0.8912\n",
      "val Loss: 0.1902 Acc: 0.9525\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.263107 --> 0.190202).  Saving model ...\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.2996 Acc: 0.9061\n",
      "val Loss: 0.2002 Acc: 0.9421\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.2529 Acc: 0.9204\n",
      "val Loss: 0.1416 Acc: 0.9672\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.190202 --> 0.141557).  Saving model ...\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.2329 Acc: 0.9270\n",
      "val Loss: 0.1096 Acc: 0.9728\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.141557 --> 0.109588).  Saving model ...\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.2014 Acc: 0.9357\n",
      "val Loss: 0.1299 Acc: 0.9652\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.2052 Acc: 0.9331\n",
      "val Loss: 0.1090 Acc: 0.9692\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.109588 --> 0.108967).  Saving model ...\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.1663 Acc: 0.9484\n",
      "val Loss: 0.1054 Acc: 0.9716\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.108967 --> 0.105377).  Saving model ...\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.1491 Acc: 0.9553\n",
      "val Loss: 0.0969 Acc: 0.9732\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.105377 --> 0.096872).  Saving model ...\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.1427 Acc: 0.9554\n",
      "val Loss: 0.0950 Acc: 0.9700\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.096872 --> 0.095001).  Saving model ...\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.1207 Acc: 0.9630\n",
      "val Loss: 0.1226 Acc: 0.9612\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.1372 Acc: 0.9584\n",
      "val Loss: 0.0910 Acc: 0.9816\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.095001 --> 0.090957).  Saving model ...\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.1055 Acc: 0.9701\n",
      "val Loss: 0.0849 Acc: 0.9780\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.090957 --> 0.084931).  Saving model ...\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.1300 Acc: 0.9589\n",
      "val Loss: 0.0907 Acc: 0.9780\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.1121 Acc: 0.9659\n",
      "val Loss: 0.0904 Acc: 0.9796\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0981 Acc: 0.9700\n",
      "val Loss: 0.0768 Acc: 0.9860\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.084931 --> 0.076805).  Saving model ...\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0999 Acc: 0.9686\n",
      "val Loss: 0.0756 Acc: 0.9824\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.076805 --> 0.075645).  Saving model ...\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.1033 Acc: 0.9698\n",
      "val Loss: 0.0713 Acc: 0.9832\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.075645 --> 0.071335).  Saving model ...\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0897 Acc: 0.9718\n",
      "val Loss: 0.0726 Acc: 0.9864\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0919 Acc: 0.9719\n",
      "val Loss: 0.0673 Acc: 0.9828\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.071335 --> 0.067341).  Saving model ...\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0831 Acc: 0.9758\n",
      "val Loss: 0.0717 Acc: 0.9844\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0837 Acc: 0.9761\n",
      "val Loss: 0.0710 Acc: 0.9856\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0729 Acc: 0.9767\n",
      "val Loss: 0.0657 Acc: 0.9852\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.067341 --> 0.065672).  Saving model ...\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0732 Acc: 0.9763\n",
      "val Loss: 0.0680 Acc: 0.9860\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0748 Acc: 0.9764\n",
      "val Loss: 0.0788 Acc: 0.9808\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0599 Acc: 0.9817\n",
      "val Loss: 0.0704 Acc: 0.9880\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 3 out of 11\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9747\n",
      "val Loss: 0.0678 Acc: 0.9856\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 4 out of 11\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0730 Acc: 0.9767\n",
      "val Loss: 0.0691 Acc: 0.9880\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 5 out of 11\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0730 Acc: 0.9762\n",
      "val Loss: 0.1275 Acc: 0.9628\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 6 out of 11\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9742\n",
      "val Loss: 0.0659 Acc: 0.9892\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 7 out of 11\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0538 Acc: 0.9849\n",
      "val Loss: 0.0730 Acc: 0.9856\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 8 out of 11\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0545 Acc: 0.9828\n",
      "val Loss: 0.0726 Acc: 0.9832\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 9 out of 11\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0511 Acc: 0.9830\n",
      "val Loss: 0.0724 Acc: 0.9848\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 10 out of 11\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0669 Acc: 0.9787\n",
      "val Loss: 0.0639 Acc: 0.9860\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.065672 --> 0.063897).  Saving model ...\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0544 Acc: 0.9823\n",
      "val Loss: 0.0633 Acc: 0.9884\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.063897 --> 0.063261).  Saving model ...\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0547 Acc: 0.9825\n",
      "val Loss: 0.0687 Acc: 0.9840\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0599 Acc: 0.9816\n",
      "val Loss: 0.0735 Acc: 0.9876\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0535 Acc: 0.9845\n",
      "val Loss: 0.0636 Acc: 0.9880\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 3 out of 11\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0690 Acc: 0.9785\n",
      "val Loss: 0.0691 Acc: 0.9876\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 4 out of 11\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0468 Acc: 0.9856\n",
      "val Loss: 0.0679 Acc: 0.9848\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 5 out of 11\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0600 Acc: 0.9803\n",
      "val Loss: 0.0658 Acc: 0.9884\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 6 out of 11\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0427 Acc: 0.9871\n",
      "val Loss: 0.0623 Acc: 0.9904\n",
      "Current learning rate : 0.00100000\n",
      "Validation loss decreased (0.063261 --> 0.062330).  Saving model ...\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0407 Acc: 0.9861\n",
      "val Loss: 0.0648 Acc: 0.9900\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 1 out of 11\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0384 Acc: 0.9863\n",
      "val Loss: 0.0669 Acc: 0.9888\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 2 out of 11\n",
      "Epoch 49/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0411 Acc: 0.9861\n",
      "val Loss: 0.0655 Acc: 0.9888\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 3 out of 11\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0537 Acc: 0.9818\n",
      "val Loss: 0.0806 Acc: 0.9852\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 4 out of 11\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 0.9887\n",
      "val Loss: 0.0651 Acc: 0.9892\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 5 out of 11\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0515 Acc: 0.9831\n",
      "val Loss: 0.0684 Acc: 0.9876\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 6 out of 11\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0476 Acc: 0.9840\n",
      "val Loss: 0.0690 Acc: 0.9888\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 7 out of 11\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0400 Acc: 0.9883\n",
      "val Loss: 0.0645 Acc: 0.9896\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 8 out of 11\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0301 Acc: 0.9910\n",
      "val Loss: 0.0684 Acc: 0.9912\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 9 out of 11\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0316 Acc: 0.9894\n",
      "val Loss: 0.0737 Acc: 0.9840\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 10 out of 11\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0301 Acc: 0.9909\n",
      "val Loss: 0.0682 Acc: 0.9884\n",
      "Current learning rate : 0.00100000\n",
      "EarlyStopping counter: 11 out of 11\n",
      "Early stopping\n",
      "Training complete in 60m 10s\n",
      "Best val Acc: 0.991211\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_ft = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, 'lotte_model_resnext2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel = torch.load('lotte_model_resnext2.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test images: 99.01 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in teLoaders:\n",
    "        images, labels = data\n",
    "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
    "        \n",
    "        outputs = testModel(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of test images: %.2f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'ID_gum', 1: 'buttering', 2: 'couque_coffee', 3: 'chocopie', 4: 'cidar', 5: 'couque_white', 6: 'coke', 7: 'diget_ori', 8: 'diget_choco', 9: 'gumi_gumi', 10: 'homerunball', 11: 'jjolbyung_noodle', 12: 'juicyfresh', 13: 'jjolbyung_ori', 14: 'spearmint', 15: 'squid_peanut', 16: 'samdasu', 17: 'tuna', 18: 'toreta', 19: 'vita500', 20: 'welchs', 21: 'zec'}\n"
     ]
    }
   ],
   "source": [
    "tag_classes = ['ID_gum', 'buttering', 'couque_coffee', 'chocopie', 'cidar', 'couque_white', 'coke', 'diget_ori', 'diget_choco', 'gumi_gumi', 'homerunball', 'jjolbyung_noodle', 'juicyfresh', 'jjolbyung_ori', 'spearmint', 'squid_peanut', 'samdasu', 'tuna', 'toreta', 'vita500', 'welchs', 'zec']\n",
    "tag_dict = dict()\n",
    "for i, label in enumerate(tag_classes):\n",
    "    tag_dict[i] = label\n",
    "\n",
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ID_gum : 98.71245 %\n",
      "Accuracy of buttering : 100.00000 %\n",
      "Accuracy of homerunball : 100.00000 %\n",
      "Accuracy of jjolbyung_noodle : 100.00000 %\n",
      "Accuracy of juicyfresh : 100.00000 %\n",
      "Accuracy of jjolbyung_ori : 100.00000 %\n",
      "Accuracy of spearmint : 98.27586 %\n",
      "Accuracy of squid_peanut : 100.00000 %\n",
      "Accuracy of samdasu : 100.00000 %\n",
      "Accuracy of  tuna : 99.66443 %\n",
      "Accuracy of toreta : 100.00000 %\n",
      "Accuracy of vita500 : 100.00000 %\n",
      "Accuracy of couque_coffee : 99.56897 %\n",
      "Accuracy of welchs : 100.00000 %\n",
      "Accuracy of   zec : 98.98649 %\n",
      "Accuracy of chocopie : 98.59649 %\n",
      "Accuracy of cidar : 99.60317 %\n",
      "Accuracy of couque_white : 99.19355 %\n",
      "Accuracy of  coke : 99.22481 %\n",
      "Accuracy of diget_ori : 93.98496 %\n",
      "Accuracy of diget_choco : 94.86405 %\n",
      "Accuracy of gumi_gumi : 100.00000 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0 for i in range(len(class_names)))\n",
    "class_total = list(0 for i in range(len(class_names)))\n",
    "with torch.no_grad():\n",
    "    for data in teLoaders:\n",
    "        images, labels = data\n",
    "        images, labels = Variable(images.float().cuda()), Variable(labels.float().cuda())\n",
    "        \n",
    "        outputs = testModel(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "\n",
    "        for i in range(c.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[int(label.item())] += c[i].item()\n",
    "            class_total[int(label.item())] += 1\n",
    "\n",
    "for i in range(len(class_names)):\n",
    "    print('Accuracy of %5s : %.5f %%' % (tag_dict[int(class_names[i])], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
