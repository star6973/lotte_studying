### VGG-F, VGG-M, VGG-S의 구조

- VGG-F, VGG-M, VGG-S는 속도와 정확도의 트레이드오프를 고려한 모델들이다. VGG-F는 fast, 즉 빠름에 초점을 맞춘 모델이고, VGG-S는 slow, 즉 느리지만 정확도에 초점을 맞춘 모델이다. VGG-M은 medium으로 이 둘의 중간 정도라고 보면 된다

![VGG](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbOgKcE%2FbtqwkWmeIZS%2F2WpkIhZ3pDWqw3CtXseK31%2Fimg.png)

- 0: VGG-F는 224x224x3사이즈의 이미지를 입력받는다 

- 첫번째 레이어[convolution] : 64개의 11x11x3사이즈 필터커널로 stride =4, zero-padding = none으로 입력영상을 컨볼루션해준다 그 뒤 LRN[local response nomalization - 수렴속도 향상 ]을 시행한다. 그 뒤 max-pooling으로 2만큼 다운 샘플링을 한다 
결국 27x27사이즈의 특성맵에 64장으로 된다 

- 두번째 레이어(컨볼루션 레이어, conv2): 256개의 5x5x64 사이즈의 필터커널로 특성맵을 컨볼루션 해준다. stride = 1 , zero-padding = 2로 설정하며 max-pooling =2 로 하여 
256개의 13x13의 특성맵을 얻는다 


- 세번째 레이어(컨볼루션 레이어, conv3): 256개의 3 x 3 x 256 사이즈 필터커널로 특성맵을 컨볼루션해준다. 컨볼루션 보폭은 1로 설정하고, zero-padding은 1만큼 해준다. 결과적으로 13 x 13 사이즈의 256장의 특성맵이 산출된다. ReLU활성화 함수를 적용한다. 

 

- 네번째 레이어(컨볼루션 레이어, conv4): 256개의 3 x 3 x 256 사이즈 필터커널로 특성맵을 컨볼루션해준다. 컨볼루션 보폭은 1로 설정하고, zero-padding은 1만큼 해준다. 결과적으로 13 x 13 사이즈의 256장의 특성맵이 산출된다. ReLU활성화 함수를 적용한다. 

- 다섯번째 레이어[컨볼루션] : 256개의 3x3x256 사이즈 필터커널을 특성맵을 만들어준다. stride =1, zero-padding = 1로 하며 max-pooling = 2로 하여 
256장의 6x6사이즈의 특성맵을 갖는다.

- 여섯번째 레이어(fully-connected 레이어, full6): 전 층에서 생성된 6 x 6 x 256의 특성맵을 flatten 해준다. flatten이란 전 층의 아웃풋을 1차원의 벡터로 펼쳐주는 것을 의미한다. 따라서 6 x 6 x 256 = 9216개의 뉴런이 되고 full6층의 4096개 뉴런과 fully connected된다. 4096개의 뉴런으로 출력된 것을 ReLU 함수로 활성화해준다. 과적합을 막기 위해 full6에는 훈련시 dropout이 적용된다


- 일곱번째 레이어(fully-connected 레이어, full7): 4096개의 뉴런으로 구성되어 있다. full6의 4096개 뉴런과 fully-connected(전 층과 이번 층의 모든 뉴런이 서로 다 연결)되어 있다. 출력된 값을 역시 ReLU 함수로 활성화해준다. full7에는 훈련시 dropout이 적용된다.

- 여덟번째 레이어(fully-connected 레이어, full8): 1000개의 뉴런으로 구성되어 있다. full7의 4096개 뉴런과 fully-connected 되어 있다. 출력된 값은 softmax 함수로 활성화되어 입력된 이미지가 어느 클래스에 속하는지 알 수 있게 해준다. 


### 왜 VGG-F가 가장 빠른가 

- VGG-F는 첫번째 레이어에서 입력영상을 64개의 11 x 11 x 3 사이즈 필터커널로 보폭 4로 컨볼루션해주는 반면, VGG-M은 첫번째 레이어에서 입력영상을 96개의 7 x 7 x 3 사이즈 필터커널로 보폭 2로 컨볼루션해준다. 필터커널의 사이즈가 작을수록, 또한 보폭이 작을수록 연산량은 많아진다. 

- 이번에는 VGG-M과 VGG-S의 연산량을 비교해보자. 연산량에 가장 큰 차이를 유발하는 곳은 바로 두번째 레이어다. 두번째 레이어에서 VGG-S는 VGG-M이 보폭2로 컨볼루션을 진행하는 것과 달리 보폭1로 컨볼루션을 진행한다. 





## VGGNet

- VGG팀은 깊이의 영향만을 최대한 확인 하고자 컨볼루션 필터커널의 사이즈는 가장 작은 3x3으로 고정했다.

- 개인적으로 나는 모든 필터 커널의 사이즈를 3 x 3으로 설정했기 때문에 네트워크의 깊이를 깊게 만들 수 있다고 생각한다. 왜냐하면 필터커널의 사이즈가 크면 그만큼 이미지의 사이즈가 금방 축소되기 때문에 네트워크의 깊이를 충분히 깊게 만들기 불가능하기 때문이다. 

- VGG 연구팀은 original 논문에서 총 6개의 구조(A, A-LRN, B, C, D, E)를 만들어 성능을 비교했다. 여러 구조를 만든 이유는 기본적으로 깊이의 따른 성능 변화를 비교하기 위함이다. 이중 D 구조가 VGG16이고 E 구조가 VGG19라고 보면 된다. 

![VGG](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb1Vk5P%2FbtqwqjujKsa%2FTL2HyQ4kj6pNPz4TsirknK%2Fimg.png)


- VGG 연구팀은 AlexNet과 VGG-F, VGG-M, VGG-S에서 사용되던 Local Response Normalization(LRN)이 A 구조와 A-LRN 구조의 성능을 비교함으로 성능 향상에 별로 효과가 없다고 실험을 통해 확인했다. 그래서 더 깊은 B, C, D, E 구조에는 LRN을 적용하지 않는다고 논문에서 밝혔다. 또한 그들은 깊이가 11층, 13층, 16층, 19층으로 깊어지면서 분류 에러가 감소하는 것을 관찰했다. 즉, 깊어질수록 성능이 좋아진다는 것이었다. 



#### 필터 크기

- 3 x 3 필터로 세 차례 컨볼루션을 하는 것이 7 x 7 필터로 한 번 컨볼루션하는 것보다 나은 점은 무엇일까? 일단 가중치 또는 파라미터의 갯수의 차이다. 3 x 3 필터가 3개면 총 27개의 가중치를 갖는다. 반면 7 x 7 필터는 49개의 가중치를 갖는다. CNN에서 가중치는 모두 훈련이 필요한 것들이므로, 가중치가 적다는 것은 그만큼 훈련시켜야할 것의 갯수가 작아진다. 따라서 학습의 속도가 빨라진다. 동시에 층의 갯수가 늘어나면서 특성에 비선형성을 더 증가시키기 때문에 특성이 점점 더 유용해진다. 

![filterscale](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FT5kb2%2FbtqwonLQrNm%2Fd56nskDXEYoCWRDpxxkeAK%2Fimg.png)

- 1) 1층(conv1_1): 64개의 3 x 3 x 3 필터커널로 입력이미지를 컨볼루션해준다. zero padding은 1만큼 해줬고, 컨볼루션 보폭(stride)는 1로 설정해준다. zero padding과 컨볼루션 stride에 대한 설정은 모든 컨볼루션층에서 모두 동일하니 다음 층부터는 설명을 생략하겠다. 결과적으로 64장의 224 x 224 특성맵(224 x 224 x 64)들이 생성된다. 활성화시키기 위해 ReLU 함수가 적용된다. ReLU함수는 마지막 16층을 제외하고는 항상 적용되니 이 또한 다음 층부터는 설명을 생략하겠다. 

-  2) 2층(conv1_2): 64개의 3 x 3 x 64 필터커널로 특성맵을 컨볼루션해준다. 결과적으로 64장의 224 x 224 특성맵들(224 x 224 x 64)이 생성된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용함으로 특성맵의 사이즈를 112 x 112 x 64로 줄인다. 


- 3) 3층(conv2_1): 128개의 3 x 3 x 64 필터커널로 특성맵을 컨볼루션해준다. 결과적으로 128장의 112 x 112 특성맵들(112 x 112 x 128)이 산출된다. 

 

- 4) 4층(conv2_2): 128개의 3 x 3 x 128 필터커널로 특성맵을 컨볼루션해준다. 결과적으로 128장의 112 x 112 특성맵들(112 x 112 x 128)이 산출된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용해준다. 특성맵의 사이즈가 56 x 56 x 128로 줄어들었다.

 

- 5) 5층(conv3_1): 256개의 3 x 3 x 128 필터커널로 특성맵을 컨볼루션한다. 결과적으로 256장의 56 x 56 특성맵들(56 x 56 x 256)이 생성된다. 

 

- 6) 6층(conv3_2): 256개의 3 x 3 x 256 필터커널로 특성맵을 컨볼루션한다. 결과적으로 256장의 56 x 56 특성맵들(56 x 56 x 256)이 생성된다. 

 

- 7) 7층(conv3_3): 256개의 3 x 3 x 256 필터커널로 특성맵을 컨볼루션한다. 결과적으로 256장의 56 x 56 특성맵들(56 x 56 x 256)이 생성된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용한다. 특성맵의 사이즈가 28 x 28 x 256으로 줄어들었다. 

 

- 8) 8층(conv4_1): 512개의 3 x 3 x 256 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 28 x 28 특성맵들(28 x 28 x 512)이 생성된다. 

 

- 9) 9층(conv4_2): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 28 x 28 특성맵들(28 x 28 x 512)이 생성된다. 

 

- 10) 10층(conv4_3): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 28 x 28 특성맵들(28 x 28 x 512)이 생성된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용한다. 특성맵의 사이즈가 14 x 14 x 512로 줄어든다.

 

- 11) 11층(conv5_1): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 14 x 14 특성맵들(14 x 14 x 512)이 생성된다.

 

- 12) 12층(conv5_2): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 14 x 14 특성맵들(14 x 14 x 512)이 생성된다.

 

- 13) 13층(conv5-3): 512개의 3 x 3 x 512 필터커널로 특성맵을 컨볼루션한다. 결과적으로 512장의 14 x 14 특성맵들(14 x 14 x 512)이 생성된다. 그 다음에 2 x 2 최대 풀링을 stride 2로 적용한다. 특성맵의 사이즈가 7 x 7 x 512로 줄어든다.

 

- 14) 14층(fc1): 7 x 7 x 512의 특성맵을 flatten 해준다. flatten이라는 것은 전 층의 출력을 받아서 단순히 1차원의 벡터로 펼쳐주는 것을 의미한다. 결과적으로 7 x 7 x 512 = 25088개의 뉴런이 되고, fc1층의 4096개의 뉴런과 fully connected 된다. 훈련시 dropout이 적용된다.

 

- 15) 15층(fc2): 4096개의 뉴런으로 구성해준다. fc1층의 4096개의 뉴런과 fully connected 된다. 훈련시 dropout이 적용된다. 

 
- 16) 16층(fc3): 1000개의 뉴런으로 구성된다. fc2층의 4096개의 뉴런과 fully connected된다. 출력값들은 softmax 함수로 활성화된다. 1000개의 뉴런으로 구성되었다는 것은 1000개의 클래스로 분류하는 목적으로 만들어진 네트워크란 뜻이다. 








