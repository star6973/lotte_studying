## 논문에서의 앙상블 

- high level feature 
더 높은 차원의 feature 

- low level feature  
텍스쳐 같은 느낌  

## 앙상블
- 앙상블 기법은 weak learner 을 잘 조합해서 strong learner를 만드는것을 목표로 한다 

- 현실에서의 기능은 개개인의 능력을 갖고있는 팀원들을 모아 개인이서 할수없는일을 해내는 것이 앙상블이다 


>> 앙상블기법에는 대표적으로 bagging과 boosting이 있다

- bagging은 기본 데이터셋을 샘플링하여 n개의 데이터셋을 만들어서 n개의 모델을 학습시켜 최종 결정을 하는 모델이다. 샘플링을 마치고 나면 n개의 모델이 독립적으로 동시에 각각의 데이터셋을 학습할 수 있으므로 속도가 빠르다. 이 때문에 병렬적이라고 이야기한다.



- boosting에서는 첫번째 모델이 기본 데이터셋을 그대로 학습을 한다. 그 다음 두번째 모델은 전체 데이터를 학습하되 첫번째 모델이 에러를 일으킨 데이터에 더 큰 중점을 두고 학습을 진행한다. 세번째는 앞의 두 모델이 힘을 합쳐도 맞추지 못한 데이터에 중점을 두고 학습을 진행한다. 앞 모델의 학습이 끝나야 뒷 모델이 그 결과를 기반으로하여 가중치를 결정하고 학습을 할 수 있기 때문에 순차적으로 학습을 해야한다. 이 때문에 직렬적이라고 이야기하며 상대적으로 속도가 느리다.


- 예를 들면 bagging은 10000개의 데이터를 2000개씩 5조로 나누어 5개의 모델이 병렬로 학습을 진행하며 boosting은 5개의 모델이 모두 10000개의 데이터를 학습하되 어느 데이터를 중점으로 학습할지에 차이가 있고, 그 결정은 앞 모델들의 학습결과에 기반하여 결정한다.

#### 모델의 bias와 variation(편의상 아래에서는 task는 classification이라고 생각하고 서술)

- 알고리즘의 bias가 크다는 것은 모델이 정상적으로 학습을 마친 후 정확도가 기대치보다 떨어지는 것을 말한다. 이는 under fitting이라고도 생각할 수 있다. 구체적으로는 이상적인 decision boundary에 비해 지나치게 단순한 decision boundary(예를 들어 직선)를 사용한 경우이다.

![dkdtkdqmf](https://t1.daumcdn.net/cfile/tistory/997396365BA26D2C35)


- 알고리즘의 variation이 크다는 것은 정상적으로 학습을 마친 후 새로운 데이터셋에 대해서는 그 결과가 매우 다르게 나오는 것을 이야기한다. 이는 overfitting이라고 생각할 수 있다. 구체적으로는 이상적인 decision boundary보다 지나치게 복잡한 decision boundary(복잡한 곡선)를 사용한 경우이다.

![dkdtkd](https://t1.daumcdn.net/cfile/tistory/99AA05365BA26D2E2F)

