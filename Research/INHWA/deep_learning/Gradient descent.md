우리가 원하는 것은 이 신경망에 학습 데이터 전체를 집어넣는 알고리즘입니다

MNIST data에는 손으로 쓰여진 많은 숫자들과 그 숫자들이 원래 무엇이었는지 알려주는 라벨이 포함되어있고
신경망 합습을 통해 13000여개의 가중치및 바이어스를 조정함으로써 성능이 개선될 것 입니다

하지만 우리는 훈련데이터를 넘어서서 실제데이터의 성능을 알아야하기 때문에 갖고있는 데이터를 2분할 합니다 . 훈련데이터와 테스트 데이터.

## Cost function

- 요약하자면 기계학습은 근본적으로 특정한 함수의 최소값을 얻는 일이다.  

각 뉴런이 이전 층의 모든 뉴련과 연결되어있고 각각의 활성화를 결정하는 가중 합계의 가중치는 그 연결으 세기입니다. 

그래서 우리는 가중치와 바이어스를 완전히 무작위로 설정할것입니다. 물론 처음은 엉망입니다

그래서 우리는 Cost함수를 컴퓨터에 정의해줄겁니다 .

Cost함수는 라벨값과의 제곱의 차 = 0으로 수렴하게 할것입니다

Cost =  y(w) - label

 

[0.32 - 0]^2  
[0.76 - 0]^2  
[0.16 - 1]^2 >> 차이를 0으로 만드려하니 2번값은 1로 수렴하게 하고 나머지들은 0으로 수렴하게 한다  
[0.32 - 0]^2  
[0.35 - 0]^2  
[0.32 - 0]^2  
[0.76 - 0]^2  
[0.56 - 0]^2  
[0.32 - 0]^2  
[0.65 - 0]^2  

수만가지의 학습예시 전체에 대한 평균Cost를 검토를 한다. 평균 cost는 신경망이 얼마나 엉망인지를 보여주는 지표입니다

평균 cost의 인풋값은 13000개입니다.

그래서 최저값을 구하는것은 미분값이 0이 되는값을 구하면 되는 것입니다 . 이렇게 미분값이 0인지점을 찾으면 13000차원의 그래프의 minimum을 찾습니다

그렇게 들어가는 수치미분(gradient descent)가 가는 방향은 그레디언트를 사용합니다

함수가 어디로 움직여야할지. 그곳을 찾아주는 것이 그레디언트입니다 이 그레디언트를 이용해 음의 방향을 찾고 그곳이 곧 가장 가파른 지점들이기에 국소적인 최소값을 구할수있게됩니다

 이 그레디언트는 결국 최소값을 찾게 되고 코스트가 0에 가깝게 움직이게된다면 결국 W, b 이 두가지는 훈련데이터 '2'를 구별할수있는 값으로 변경되는것입니다
 
### Back-propagation

활성도를 높이기 위해 함께 조화를 이룰 수 있는 세가지 방법이 있습니다.

- 바이어스 증가
- 가중치를 증가
- 이전 레이어의 활성도 변경

가중치를 조정하는 방법에만 초점을 맞추고 가중치의 실제 영향 수준이 다른지 확인해보면

a(Wx + b)  

a=활성화함수 ,W = 웨이트 , b= 바이어스 , x = 인풋값

이 값이 크면 뉴런이 활성화가 되는데 ,방법중 1개는 활성화함수를 바꿔주는 것이다.
 
활성화함수가 바뀌게 되면 결국 전체값이 변경이 되어 W과 b값을 간접적으로 영향을 줄수 있는것이다
 
 
y = Wx + b 

뉴런의 기울기는 x , 즉 x가 크면 그 다음단의 뉴런에 미치는 영향의 크기, 
즉 다음단의 뉴런과의 연결 세기입니다.

비용함수[코스트]는 많은 예제의 비용들을 전부 평균 낸 것과 관련있기때문에 
이전단의 x값[a(x-1)]으로 Cost를 미분한다면 이것은 이전층의 활성화정도애 얼마나 민감한지를 알수있스빈다
 











###Stochastic gradient descent(SGD)

- 엄청나게 많은 모든 데이터의 가중치을 조절하며 다보고 가는것보다 미니배치의 cost를 구하고 learning rate를 곱해 그 방향으로 가는 것을 반복하여 계산횟수가 줄어든다

- 결국 모든 데이터의 그레디언트값이 아니기에 정방향으로 나가아진 않지만 결국 같은 지점으로 통한다 
![SGD](https://t1.daumcdn.net/cfile/tistory/996AFC3C5B0CF0C901) 

###Momentum(모멘텀)

- 이전에 업데이트된 량을 남겨두고 모멘텀을 더해준다 그래서 연산 수행 반복 횟수를 줄어든다 . 
경사의 로스는 로컬 미니멈에 멀수록 크지만 가까워질수록 엄청나게 작아진다 그래서 미니배치를 통해 빠르게 다가가서 연산 수행 횟수를 줄어들게한다.


![SGD](https://t1.daumcdn.net/cfile/tistory/99A14F455B0CF54C21) 
 
m의 정확한 용어는 아니지만 저희는 그냥 모멘텀(운동량) 또는 모멘텀 계수라고 부릅니다. 보통 0.9로 설정하며 교차 검증을 한다면 0.5에서 시작하여 0.9, 0.95, 0.99 순서로 증가시켜 검증합니다

### Nesterov Accelrated Gradient(NAG, 네스테로프 모멘텀)

- 모멘텀 방향과 이전의 그래디언트 방향 , 현재의 그래디언트 방향 + 모멘트 방향을 더해서 빠르게 다가간다.
미리 예측을 해서 더 미리 빠른 효과를 얻는다

![NAG](https://t1.daumcdn.net/cfile/tistory/996E494B5B0D03A003)
![NAG-equation](https://t1.daumcdn.net/cfile/tistory/99527A335B0CFF2E2B)

